Under the hood
##############

This chapter gives insights into the inner functioning of the Genode OS
framework. In particular, it explains how the concepts explained in Chapter
[Architecture] are realized on different kernels and hardware platforms.


Component-local startup code and linker scripts
===============================================

All Genode components including core rely on the same startup code, which
is roughly outlined at the end of Section [Component creation]. This
section revisits the steps in more detail and refers to the corresponding
points in the source code. Furthermore, it provides background information
about the linkage of components, which is closely related to the startup
code.


Linker scripts
~~~~~~~~~~~~~~

Under the hood, the Genode build system uses three different linker scripts
located at _repos/base/src/platform/_:

:genode.ld: used for statically linked components, including
  the core component,

:genode_dyn.ld: used for dynamically linked components, i.e., components
  that are linked against at least one shared library,

:genode_rel.ld: used for shared libraries.

Additionally, there exists a special linker script for the dynamic linker
(Section [Dynamic linker]).

Each program image generated by the linker generally consists of three parts,
which appear consecutively in the component's virtual memory.

# A read-only "text" part contains sections for code, read-only
  data, and the list of global constructors and destructors.

  The startup code is placed in a dedicated section '.text.crt0', which
  appears right at the start of the segment. Thereby the link address of
  the component is known to correspond with the ELF entrypoint (the first
  instruction of the assembly startup code).
  This is useful when converting the ELF image of the base-hw version of
  core into a raw binary. Such a raw binary can be loaded directly into
  the memory of the target platform without the need for an ELF loader.

  The mechanisms for constructing the list of constructors and destructors
  differs between the CPU architecture and are defined by the architecture's
  ABI. On x86, the lists are represented by '.ctors.*' and '.dtors.*'.
  On ARM, the information about global constructors is represented by
  '.init_array' and there is no visible information about global destructors.

# A read-writable "data" part that is pre-populated with data.

# A read-writable "bss" part that is not physically present in the binary but
  known to be zero-initialized when the ELF image is loaded.

The link address is not defined in the linker script but specified as
linker argument. The default link address is specified in a platform-specific
spec file, e.g., _repos/base-nova/mk/spec-nova.mk_ for the NOVA platform.
Components that need to organize their virtual address space in a special
way (e.g., a virtual machine monitor that co-locates the guest-physical
address space with its virtual address space) may specify link addresses
that differ from the default by overriding the LD_TEXT_ADDR value.


ELF entry point
---------------

As defined at the start of the linker script via the ENTRY directive, the
ELF entrypoint is the function '_start'. This function is located at the very
beginning of the '.text.crt0' section. See the Section [Startup code] for
more details.


Symbols defined by the linker script
------------------------------------

The following symbols are defined by the linker script and used by the
base framework.

:'_prog_img_beg, _prog_img_data, _prog_img_end':
  Those symbols mark the start of the "text" part, the start of the "data"
  part (the end of the "text" part), and the end of the "bss" part.
  They are used by core to exclude those virtual memory ranges from
  the core's virtual-memory allocator (core-region allocator).

:'_parent_cap, _parent_cap_thread_id, _parent_cap_local_name':
  Those symbols are located at the beginning of the "data" part.
  During the ELF loading of a new component, the parent writes
  information about the parent capability to this location (the start
  of the first read-writable ELF segment). See the corresponding code
  in the '_setup_elf' function in _base/src/base/process/process.cc_.
  The use of the information depends on the used base platforms. E.g.,
  on a platform where a capability is represented by a tuple of a global
  thread ID and an object ID such as OKL4 and L4ka::Pistachio, those
  information are taken as verbatim values. On platforms that fully
  support capability-based security without the use of any form of
  a global name to represent a capability, the information are unused.
  Here, the parent capability is represented with the same known
  local name in all components.

Even though the linker scripts are used across all base platforms, they
contain a few platform-specific supplements that are needed to support
the respective kernel ABIs. For example, the definition of the symbol
'__l4sys_invoke_indirect' is needed only on the Fiasco.OC platform and
is unused on the other base platforms. Please refer to the comments
in the linker script for further explanations.


Startup code
~~~~~~~~~~~~

The execution the initial thread of a new component starts at the ELF
entry point, which corresponds to the '_start' function. This is an
assembly function defined in _repos/base/platform/<arch>/crt0.s_ where
_<arch>_ is the CPU architecture (x86_32, x86_64, or ARM).


Assembly startup code
---------------------

The assembly startup code is position-independent code (PIC).
Because the Genode base libraries are linked against both statically-linked
and dynamically linked executables, they have to be compiled as PIC code.
To be consistent with the base libraries, the startup code needs to be
position-independent, too.

The code performs the following steps:

# Saving the initial state of certain CPU registers. Depending on the
  used kernel, these registers are used to pass information from the
  kernel to the core component. More details about this information
  are provided by Section [Bootstrapping and allocator setup]. The
  initial register values are saved in global variables named
  '_initial_<register>'. The global variables are located in the BSS
  segment. Note that those variables are used solely by core.

# Setting up the initial stack. Before the assembly code can call any
  higher-level C function, the stack pointer must be initialized to
  point the top of a valid stack. The initial stack is located in the
  BSS section and referred to by the symbol '_stack_high'. However,
  having a stack located within the BSS section is dangerous. If it
  overflows (e.g., by declaring large local variables, or recursive
  function calls), the stack would silently overwrite parts of the
  BSS and DATA sections located below the lower stack boundary. For prior
  known code, the stack can be dimensioned to a reasonable size. But
  for arbitrary application code, no assumption about
  the stack usage can be made. For this reason, the initial stack cannot
  be used for the entire lifetime of the component. Before any
  component-specific code is called, the stack needs to be relocated to
  another area of the virtual address space where the lower bound of the
  stack is guarded by empty pages. When using such a "real" stack, a
  stack overflow will produce a page fault, which can be handled or at least
  immediately detected. The initial stack is solely used to perform the
  steps needed to set up the real stack. Because those steps are the same for
  all components, the usage of the initial stack is bounded.

# Because the startup code used by statically linked components as well as
  the dynamic linker, the startup immediately calls the 'init_rtld' hook
  function.
  For regular components, the function does not do anything. The default
  implementation in _repos/base/src/platform/init_main_thread.cc_ is a weak
  function. The dynamic linker provides a non-weak implementation, which
  allows the linker to perform initial relocations of itself very early at
  the dynamic linker's startup.

# By calling the 'init_main_thread' function defined in
  _repos/base/src/platform/init_main_thread.cc_, the assembly code triggers
  the execution of all the steps needed for the creation of the real stack.
  The function is implemented in C++, uses the initial stack, and returns
  the address of the real stack.

# With the new stack pointer returned by 'init_main_thread', the assembly
  startup code is able to switch the stack pointer from initial stack to
  the real stack. From this point on, stack overflows cannot easily corrupt
  any data.

# With the real stack in place, the assembly code finally passes the control
  over to the C++ startup code provided by the '_main' function.


Initialization of the real stack along with the Genode environment
------------------------------------------------------------------

As mentioned above, the assembly code calls the 'init_main_thread' function
(located in _repos/base/src/platform/init_main_thread.cc_) for setting up the
real stack for the program. For placing a stack in dedicated portion of the
component's virtual address space, the function needs to overcome two
principle problems:

* It needs to obtain the backing store used for the stack, i.e., by
  allocating a dataspace from the component's RAM session as initialized
  by the parent.

* It needs to preserve a portion of its virtual address space for placing
  the stack and make the allocated memory visible within this portion.

In order to solve both problems, the function needs to obtain capabilities
for its RAM session and RM session from its parent. This comes down to
the need for performing RPC calls. First, for requesting the RM and RAM
session capabilities from the parent, and second, for invoking the session
capabilities to perform the RAM allocation and RM attach operations.

The RPC mechanism is based on C++. In particular, the mechanism supports
the propagation of C++ exceptions across RPC interfaces. Hence,
before being able to perform RPC calls, the program must initialize
the C++ runtime including the exception support.
The initialization of the C++ runtime, in turn, requires support for
dynamically allocating memory. Hence, a heap must be available.
This chain of dependencies ultimately results in the need to construct the
entire Genode environment as a side effect of initializing the real stack of
the program.

During the construction of the Genode environment (by calling
'Genode::env()'), the program requests its own RM, RAM, CPU, and PD session
from its parent, and initializes its heap ('env()->heap()').

With the environment constructed, the program is able to interact
with its own RM and RAM sessions and can principally realize the
initialization of the real stack. However, instead of merely allocating
a new RAM dataspace and attaching the dataspace to the RM session, a
so-called thread-context area is constructed. The thread-context area
is a secondary RM session that is attached as a dataspace to the component's
actual RM session (See the description of managed dataspaces in Section
[Address-space management (RM)]).
This way, virtual-memory allocations within the thread context area can be
managed manually. I.e., the spaces between the stack of different threads are
guaranteed to remain free from any attached dataspaces.
For constructing the thread-context area, a new RM session is created
(_repos/base/src/base/context_area.cc_).


Component-specific startup code
-------------------------------

With the Genode environment constructed and the initial stack switched
to a proper stack located in the thread-context area, the component-specific
startup code of the '_main' in _repos/base/src/platform/_main.cc_ can be
executed. This code is responsible for calling the global constructors
of the program before calling program's main function.

In accordance to the established signature of the 'main' function, taking
an argument list and an environment as arguments, the startup code supplies
these arguments but uses dummy default values. However, since the values
are taken from the global variables 'genode_argv', 'genode_argc', and
'genode_envp', a global constructor is able to override the default values.

The startup code in '_main.cc' is accompanied with support for _atexit_
handling. The atexit mechanism allows for the registration of handlers
to be called at the exit of the program. It is provided in the form of
a POSIX API by the C runtime. But it is also used by the compiler to
schedule the execution of the destructors of function-local static objects.
For the latter reason, the atexit mechanism cannot be merely provided
by the (optional) C runtime but must be supported by the base library.


C++ runtime
===========

| TODO


Interaction of core with the underlying kernel
==============================================

The core is the root of the process tree. It is initialized and started
directly by the underlying kernel and has two purposes. First, it makes
the low-level physical resources of the machine available to other components
in the form of services. Those resources are the physical memory, processing
time, device resources, initial boot modules, and protection mechanisms (such
as the MMU, IOMMU, and virtualization extensions). It thereby
hides the peculiarities of the used kernel behind an API that is uniform
across all kernels supported by Genode. Core's second purpose is the
creation of the init component by using its own services and following the
steps described in Section [Component creation].

Even though core is executed in usermode, its role as the root of the
component tree makes it as critical as the kernel. It just happens to be
executed in a different processor mode. Whereas regular components solely
interact with the kernel when performing inter-component communication, core
interplays with the kernel more intensely. The following subsections go
into detail about this interplay.

The description tries to be general across the various kernels supported
by Genode. Note, however, that a particular kernel may deviate from the
general description.


Bootstrapping and allocator setup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At boot time, the kernel passes information about the physical resources and
the initial system state to core. Even though the mechanism and format of this
information varies from kernel to kernel, it generally covers the following
aspects:

* A list of free physical memory ranges
* A list of the physical memory locations of the boot modules along with their
  respective names
* The number of available CPUs
* All information needed to enable the initial thread to perform kernel
  operations


Core's allocators
-----------------

Core's kernel-specific platform initialization code (_core/platform.cc_)
uses this information to initialize the allocators used for keeping track
of physical resources. Those allocators are:

:RAM allocator: contains the ranges of the available physical memory
:I/O memory allocator: contains the physical address ranges of unused
  memory-mapped I/O resources. In general, all ranges not initially present in
  the RAM allocator are considered as I/O memory.
:I/O port allocator: contains the I/O ports on x86-based platforms that are
  currently not in use. This allocator is initialized with the entire
  I/O port range of 0 to 0xffff.
:IRQ allocator: contains the IRQs that are associated with IRQ sessions.
  This allocator is initialized with the entirety of the available IRQ
  numbers.
:Core-region allocator: contains the virtual memory regions of core that
  are not in use.

The RAM allocator and core-region allocators are subsumed in the so-called
core-memory allocator. In addition to aggregating both allocators, the
core-memory allocator allows for the allocation of core-local virtual-memory
regions that can be used for holding core-local objects. Each region
allocated from the core-memory allocator has to satisfy three conditions:

# It must be backed by a physical memory range (as allocated from the RAM
  allocator)
# It must have assigned a core-local virtual memory range (as allocated
  from the core-region allocator)
# The physical-memory range has the same size as the virtual-memory range
# The virtual memory ranges is mapped to the physical memory range using
  the MMU

Internally, the core-memory allocator maintains a so-called mapped-memory
allocator that contains ranges of ready-to-use core-local memory. If a new
allocation exceeds the available capacity, the core-memory allocator expands
its capacity by allocating a new physical memory region from the RAM
allocator, allocating a new core-virtual memory region from the core-region
allocator, and installing a mapping from the virtual region to the physical
region.

All memory allocators mentioned above are working at the granularity of
physical pages, i.e., 4 KiB.

The core-memory allocator is used as the backing store for core's heap.
It is expanded on demand but never shrunk.
This makes it unsuitable for allocating objects on the behalf of core clients
because allocations could not be reverted when closing the session.
It is solely used for dynamic memory allocations at the startup (e.g., the
memory needed for keeping the information about the boot modules),
and for keeping meta data for the allocators themselves.

; Boot modules
; ------------


Kernel-object creation
~~~~~~~~~~~~~~~~~~~~~~

Kernel objects are objects maintained within the kernel and used by the
kernel.
The exact notion of what a kernel object represents depends on the actual
kernel as the various kernels differ with respect to the abstractions they
provide.
Typical kernel objects are threads and protection domains.
Some kernels have kernel objects for memory mappings whereas others provide
page tables as kernel objects.
Whereas some kernels represent scheduling parameters as distinct kernel
objects, others subsume scheduling parameters to threads.
What all kernel objects have in common, though, is that they consume kernel
memory.
Most kernels of the L4 family preserve a fixed pool of memory for the
allocation of kernel objects.

If an arbitrary component was able to perform a kernel operation that triggers
the creation of a kernel object, the memory consumption of the kernel would
depend on the good behavior of all components. A misbehaving component may
thereby exhaust the kernel memory.

To counter this problem, on Genode, only core triggers the creation of kernel
objects and thereby guards the consumption of kernel memory. Note, however,
that not all kernels are able to prevent the creation of kernel objects
outside of core.


| TODO
|
| XXX zeroing out RAM dataspaces, keeping core's virtual memory void of non-core memory objects
| (even though core has the authority of all memory, it maps memory the
| physical pages that it needs to access)
|
| XXX page-fault handling (pager object, deferred page fault resolution)


IOMMU support
=============

As discussed in Section [Direct memory access (DMA) transactions], misbehaving
device driver may exploit DMA transactions to circumvent their component
boundary. When executing Genode on the NOVA microhypervisor, however,
bus-master DMA is subjected to the IOMMU.

NOVAs interface to the IOMMU is quite elegant. The kernel simply
applies a subset of the (MMU) address space of a process (aka protection domain
in NOVA speak) to the (IOMMU) address space of a device. So the device's
address space can be managed in the same way as we normally manage the address
space of a process. The only missing link is the assignment of device address
spaces to process address spaces. This link is provided by the dedicated system
call "assign_pci" that takes a process identifier and a device identifier as
arguments. Of course, both arguments must be subjected to a security policy.
Otherwise, any process could assign any device to any other process. To enforce
security, the process identifier is a capability to the respective protection
domain and the device identifier is a virtual address where the extended PCI
configuration space of the device is mapped in the specified protection domain.
Only if a user-level device driver got access to the extended PCI configuration
space of the device, it is able to get the assignment in place.

To make NOVA's IOMMU support available to Genode components,
the ACPI driver has the ability to hand out the extended PCI configuration
space of a device, and a NOVA-specific extension ('assign_pci') to the PD session
interface can be used to associate a PCI device with a protection domain.

; XXX [image img/iommu_aware 63%]
;   NOVAs management of the IOMMU address spaces facilities the use of
;   driver-local virtual addresses as DMA addresses.

Even though these mechanisms combined principally
suffice to let drivers operate with the IOMMU enabled, in practice, the
situation is a bit more complicated. Because NOVA uses the same
virtual-to-physical mappings for the device as it uses for the process, the DMA
addresses the driver needs to supply to the device must be virtual addresses
rather than physical addresses. Consequently, to be able to make a device
driver usable on systems without IOMMU as well as on systems with IOMMU, the
driver needs to be IOMMU-aware and distinguish both cases. This is an
unfortunate consequence of the otherwise elegant mechanism provided by NOVA. To
relieve the device drivers from caring about both cases, Genode decouples
the virtual address space of the device from the virtual address space of the
driver. The former address space is represented by a Genode process called
_device PD_. Its sole purpose
is to hold mappings of DMA buffers that are accessible by the associated
device. By using one-to-one physical-to-virtual mappings for those buffers
within the device PD, each device PD contains a subset of the physical address
space. The ACPI driver performs the assignment of device PDs to PCI
devices. If a device driver intends to use DMA, it allocates a new DMA buffer
for a specific PCI device at the ACPI driver.
The ACPI driver responds to such a request by allocating a RAM dataspace at core,
attaching it to the device PD using the dataspace's physical address as virtual
address, and handing out the dataspace capability to the client. If the driver
requests the physical address of the dataspace, the returned address will be a
valid virtual address in the associated device PD.
From this design follows that a device driver must allocate DMA buffers at the
ACPI server (specifying the PCI device the buffer is intended for) instead of
using core's RAM service to allocate buffers anonymously. Note that the
current implementation of the ACPI server assigns all PCI devices to only one
device PD.

; XXX [image img/iommu_agnostic 80%]
;   By modelling a device address space as a dedicated process (device PD),
;   the traditional way of programming DMA transactions can be maintained,
;   even with the IOMMU enabled.


Capability mechanism in depth
=============================

| TODO

; Capability representation as C++ object
; Marshalling of capabilities as RPC arguments
; Life-time management


Execution on bare hardware (base-hw)
====================================

The code specific to the base-hw platform is located within the
_repos/base-hw/_ directory. In the following description, unless explicitly
stated otherwise, all paths are relative to this directory.

In contrast to classical L4 microkernels where Genode's core process runs as
user-level roottask on top of the kernel, base-hw executes Genode's core
directly on the hardware with no distinct kernel underneath. Core and kernel
are melted into one hybrid kernel/userland program. Only a few
code paths are executed in privileged mode but most code runs in user mode.
This design has several benefits. First, the kernel part becomes much simpler.
For example, there are no allocators needed in the kernel part because
allocators are managed by the user-level part of core. Second, base-hw
side-steps long-standing hard kernel-level problems, in particular the
management of kernel resources. For the allocation of kernel objects, the
hybrid core/kernel can
employ Genode's user-level resource trading concepts as described in Section
[Resource trading]. Finally and most
importantly, merging the kernel with roottask removes a lot of
redundancies between both programs. Traditionally, both kernel and roottask
performed the book keeping of physical-resource allocations and the existence
of kernel objects such as address spaces and threads. In base-hw, those data
structures exist only once. The complexity of the combined kernel/core is
significantly lower than the sum of the complexities of a traditional
self-sufficient kernel and a distinct roottask on top. This way, base-hw helps
to make Genode's TCB less complex.

The following subsections detail the problems that base-hw had to address
to become a self-sufficient base platform for Genode.


Bootstrapping of base-hw
~~~~~~~~~~~~~~~~~~~~~~~~

A Genode-based system consists of potentially many boot modules. But
boot loaders on ARM platforms usually merely support the loading of a
single system image. Hence, base-hw requires a concept for merging
boot modules together with the core/kernel into a single image.

| XXX how does the concept works?


Kernel entry and exit
~~~~~~~~~~~~~~~~~~~~~

The execution model of the kernel can be roughly characterized as a
single-stack kernel. In contrast to traditional L4 kernels that maintain one
kernel thread per user thread, the base-hw kernel is a mere state machine
that never blocks in the kernel. State transitions are triggered by
user-level threads that enter the kernel via a system call or by device
interrupts. Once entered, the kernel applies the state change depending
on the event that caused the kernel entry, and leaves the kernel to the
user land. The transition between user and kernel mode depends on the
revision of the ARM architecture. For ARMv7, the corresponding code is
located at _src/core/spec/arm_v7/mode_transition.s_.


Interrupt handling and preemptive multi-threading
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to respond to interrupts, base-hw has to contain a driver for
the interrupt controller. ARM-based SoCs greatly differ with respect
to the used interrupt controllers. The interrupt-controller driver for
a particular SoC can be found at _src/core/include/spec/<spec>/pic.h_
and the corresponding _src/core/spec/<spec>/pic.cc_ where _<spec>_
refers to a particular platform (e.g., imx53) or an IP block that is
is used across different platforms (e.g., arm_gic for ARM's generic
interrupt controller).
Each of the drivers implement the same interface. When building core,
the build system uses the build-spec mechanism explained in
Section [Build system] to incorporate the single driver needed for the
targeted SoC.

To support preemptive multi-threading, base-hw requires a hardware timer.
The timer is programmed with the timeslice length of the currently
executed thread. Once the programed timeout elapses, the timer device
generates an interrupt that is handled by the kernel. Similarly to
interrupt controllers, there exist a variety of different timer devices
on ARM-based SoCs. Therefore, base-hw contains different timer drivers.
The timer drivers are located at _src/core/include/spec/<spec>/timer.h_
where _<spec>_ refers to the timer variant.

The in-kernel handler of the timer interrupt invokes the thread scheduler
(_src/core/include/kernel/cpu_scheduler.h_).
The scheduler maintains a list of so-called scheduling contexts where each
context refers to a thread. Each time, the kernel is entered, the scheduler
is updated with the passed duration. When updated, it takes a scheduling
decision by making the next to-be-executed thread the head of the list.
At the kernel exit, the control is passed to the user-level thread that
corresponds to the head of the scheduler list.


Split kernel interface
~~~~~~~~~~~~~~~~~~~~~~

The system-call interface of the base-hw kernel is split in two parts.
One part is usable by all components and solely contains system calls for
inter-component communication and thread synchronization. The definition
of this interface is located at _include/kernel/interface.h_. The second
part is exposed only to core. It supplements the public interface with
operations for the creation, the management, and the destruction of kernel
objects.

The distinction between both parts of the kernel interface is enforced
by the 'Thread::_call' function in _src/core/kernel/thread.cc_.


Public part of the kernel interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Threads do not run independently but interact with each other via synchronous
inter-component communication as detailed in Section
[Inter-component communication]. Within base-hw, this mechanism is referred
to as IPC (for inter-process communication).
To allow threads to perform calls to other threads or to receive RPC requests,
the kernel interface is equipped with system calls for performing IPC
(_send_request_msg_, _await_request_msg_, _send_reply_msg_.
To keep the kernel as simple as possible, IPC is performed using so-called
user-level thread-control blocks (UTCB).
Each thread has a corresponding memory page that is always
mapped in the kernel. This UTCB page is used to carry IPC payload. The largely
simplified procedure of transferring a message is as follows. (In reality, the
state space is more complex because the receiver may not be in a blocking state
when the sender issues the message)

# The user-level sender marshals its payload into its UTCB and invokes the
  kernel,
# The kernel transfers the payload from the sender's UTCB to the receiver's
  UTCB and schedules the receiver,
# The receiver retrieves the incoming message from its UTCB.

Because all UTCBs are always mapped in the kernel, no page faults can occur
during the second step. This way, the flow of execution within the kernel
becomes predictable and always returns to the user land.

In addition to IPC, threads interact via the synchronization primitives
provided by the Genode API. To implement these portions of the API, the kernel
provides system calls for managing the execution control of threads
(_pause_current_thread_, _resume_local_thread_, _yield_thread_).

To support asynchronous notifications as described in Section
[Asynchronous notifications], the kernel provides system calls for the
submission and reception of signals (_await_signal_, _signal_pending_,
_submit_signal_, and _ack_signal_) as well as the life-time management
of signal contexts (_kill_signal_context_). In contrast to other
base platforms, Genode's signal API is directly supported by the kernel
so that the propagation of signals does not require any interaction with
core's SIGNAL service (Section [Asynchonous notifications (SIGNAL)]).
However, the creation of signal contexts is arbitrated by the SIGNAL service.
This way, the kernel objects needed for the signalling mechanisms are
accounted to the corresponding clients of the SIGNAL service.


Core-private part of the kernel interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The core-private part of the kernel interface allows the user-level part
of core to perform privileged operations. Note that even though the
kernel and core are executed in different CPU modes (privileged mode and
user mode), both parts share a single address space and ultimately trust
each other. The kernel is regarded a mere support library of core that
executes those functions that can only be executed in the privileged
CPU mode. In particular, the kernel does not perform any allocation.
Instead, the allocation of a kernel objects is performed as an interplay
of core and the kernel.

# Core allocates physical memory from its physical-memory allocator.
  Most kernel-object allocations are performed in the context of one
  of core's services. Hence, those allocations can be properly accounted
  to a session quota (Section [Resource trading]). This way, kernel objects
  allocated on behalf of core's clients are "paid for" by those clients.

# Core allocates virtual memory to make the allocated physical memory visible
  within core and the kernel.

# Core invokes the kernel to construct the kernel object at the location
  specified by core. This kernel invocation is actually a system call that
  enters the kernel via the kernel-entry path.

# The kernel initializes the kernel object at the virtual address specified
  by core and returns to core via the kernel-exit path.

The core-private kernel interface consists of the following operations:

* The creation and destruction of protection domains
  (_new_pd_ and _bin_pd_), invoked by the PD service
* The creation, manipulation, and destruction of threads
  (_new_thread_, _bin_thread_, _start_thread_, _resume_thread_,
  _access_thread_regs_, and _route_thread_event_), used by the CPU service
  and the core-specific back end of the 'Genode::Thread' API
* The creation and destruction of signal receivers and signal contexts
  (_new_signal_receiver_, _bin_signal_receiver_, _new_signal_context_, and
  _bin_signal_context_), invoked by the SIGNAL service


Scheduler of the base-hw kernel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

CPU scheduling in traditional L4 microkernels is based on static priorities.
The scheduler always picks the runnable thread with highest priority for
execution.
If multiple threads share one priority, the kernel schedules those threads
in a round-robin fashion.
Whereas being pretty fast and easy to implement, this scheme has disadvantages:
First, there is no way to prevent
high-prioritized threads from starving lower-prioritized ones. Second, CPU time
could not be granted to threads and passed between them by the means of quota.
To cope with these problems without much loss of performance, base-hw employs
a custom scheduler that deviates from the traditional approach.

The base-hw scheduler introduces the distinction between high-throughput-oriented
scheduling contexts - called _fills_ - and low-latency-oriented
scheduling contexts - called _claims_. Examples for typical fills would be
the processing of a compiler job or the rendering computations of a sophisticated
graphics program. They shall obtain as much CPU time as the system can spare
but there is no demand for a high responsiveness. In contrast, a good example
for the claim category would be a typical GUI-software stack covering the
control flow from user-input drivers through a chain of GUI components to the
drivers of the graphical output. Another example is a user-level device driver
that must quickly respond to sporadic interrupts but is otherwise untrusted.
The low latency of such components is a key factor for usability and
quality of service. Besides introducing the distinction between claim and fill
scheduling contexts, base-hw introduces the notion of a so-called
_super period_, which is a multiple of typical scheduling time slices, e.g.,
one second. The entire super period
corresponds to 100% of the CPU time of one CPU. Portions of it can be assigned
to scheduling contexts. A CPU quota thereby corresponds to a percentage of the
super period.

At the beginning of a super period, each claim has its full amount of assigned
CPU quota. The priority defines the absolute scheduling order within the super
period among those claims that are active and have quota left. As long as
there exist such claims, the scheduler stays in the claim mode and the quota
of the scheduled claims decreases. At the end of a super period, the quota of
all claims gets refreshed to the initial value. Every time the scheduler can't
find an active claim with CPU-quota left, it switches to the fill mode. Fills
are scheduled in a simple round-robin fashion with identical time slices. The
proceeding of the super period doesn't affect the scheduling order and
time-slices of this mode. The concept of quota and priority that is
implemented through the claim mode aligns nicely with Genode's way of
hierarchical resource management: Through CPU-sessions, each process becomes
able to assign portions of its CPU time and subranges of its priority band to
its children without knowing the global means of CPU time or priority.


Multi-processor support of base-hw
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On uniprocessor systems, the base-hw kernel is single-threaded. Its
execution model corresponds to a mere state machine.
On SMP systems, it maintains one kernel thread and one scheduler per CPU core.
Access to kernel
objects gets fully serialized by one global spin lock that is acquired
when entering the kernel and released when leaving the kernel. This keeps the
use of multiple cores transparent to the kernel model, which greatly
simplifies the code compared to traditional L4 microkernels. Given
that the kernel is a simple state machine providing lightweight non-blocking
operations, there is little contention for the global kernel
lock. Even though this claim may not hold up when scaling to a large number of
cores, current ARM-based platforms can be accommodated well.


Cross-CPU inter-component communication
---------------------------------------

Regarding synchronous and asynchronous inter-processor communication - thanks
to the global kernel lock - the is no semantic difference to the uniprocessor
case. The only difference is that on a multiprocessor system, one processor may
change the schedule of another processor by unblocking one of its threads
(e.g., when RPC call is received by a server that resides on a different CPU
as the client).
This condition may rescind the current scheduling choice of the other processor.
To avoid lags in this case, the kernel lets the unaware target processor trap
into an inter-processor interrupt (IPI).
The targeted processor can respond to the IPI by taking the decision to
schedule the receiving thread.
As the IPI sender doesn't have to wait for an answer, the sending and
receiving CPUs remain largely decoupled.
There is no need for a complex IPI protocol between both.


TLB shootdown
-------------

With respect to the synchronization of core-local hardware, there are two
different situations to deal with. Some hardware components like most ARM
caches and branch predictors implement their own coherence protocol and thus
need adaption in terms of configuration only. Others, like the TLBs lack this
feature. When for instance a page table entry gets invalid, a TLB invalidation
of the affected entries must be done locally by each core. To signal the
necessity of TLB maintenance work, an IPI is sent to all other cores. If all
cores completed the cleaning, the thread that invoked the TLB invalidation
resumes its execution.

; Futher possible topics
; * TrustZone


Execution on the NOVA microhypervisor (base-nova)
=================================================

| TODO
; * Special linker script at base-nova/src/platform/roottask.ld - why?
; * Link to NOVA spec
; * Relationship of NOVA kernel objects to core sessions
; * Capability protection
; * Genode specific modifications of the NOVA kernel


Dynamic linker
==============

| TODO

; XXX concept of ld.lib.so
; * start ld.lib.so as regular binary, execute generic startup code
; * requesting the binary
; * lazy relocation
; * managed dataspace for keeping shared libraries (why)
; * lookup main function as present in the dynamic binary


; XXX further possible topics
; * Locking
; * C++ runtime
; * Booting

; XXX introduce the threads running within core
