Under the hood
##############

This chapter gives insights into the inner functioning of the Genode OS
framework. In particular, it explains how the concepts explained in Chapter
[Architecture] are realized on different kernels and hardware platforms.


Component-local startup code and linker scripts
===============================================

All Genode components including core rely on the same startup code, which
is roughly outlined at the end of Section [Component creation]. This
section revisits the steps in more detail and refers to the corresponding
points in the source code. Furthermore, it provides background information
about the linkage of components, which is closely related to the startup
code.


Linker scripts
~~~~~~~~~~~~~~

Under the hood, the Genode build system uses three different linker scripts
located at _repos/base/src/platform/_:

:genode.ld: used for statically linked components, including
  the core component,

:genode_dyn.ld: used for dynamically linked components, i.e., components
  that are linked against at least one shared library,

:genode_rel.ld: used for shared libraries.

Additionally, there exists a special linker script for the dynamic linker
(Section [Dynamic linker]).

Each program image generated by the linker generally consists of three parts,
which appear consecutively in the component's virtual memory.

# A read-only "text" part contains sections for code, read-only
  data, and the list of global constructors and destructors.

  The startup code is placed in a dedicated section '.text.crt0', which
  appears right at the start of the segment. Thereby the link address of
  the component is known to correspond with the ELF entrypoint (the first
  instruction of the assembly startup code).
  This is useful when converting the ELF image of the base-hw version of
  core into a raw binary. Such a raw binary can be loaded directly into
  the memory of the target platform without the need for an ELF loader.

  The mechanisms for constructing the list of constructors and destructors
  differs between the CPU architecture and are defined by the architecture's
  ABI. On x86, the lists are represented by '.ctors.*' and '.dtors.*'.
  On ARM, the information about global constructors is represented by
  '.init_array' and there is no visible information about global destructors.

# A read-writable "data" part that is pre-populated with data.

# A read-writable "bss" part that is not physically present in the binary but
  known to be zero-initialized when the ELF image is loaded.

The link address is not defined in the linker script but specified as
linker argument. The default link address is specified in a platform-specific
spec file, e.g., _repos/base-nova/mk/spec-nova.mk_ for the NOVA platform.
Components that need to organize their virtual address space in a special
way (e.g., a virtual machine monitor that co-locates the guest-physical
address space with its virtual address space) may specify link addresses
that differ from the default by overriding the LD_TEXT_ADDR value.


ELF entry point
---------------

As defined at the start of the linker script via the ENTRY directive, the
ELF entrypoint is the function '_start'. This function is located at the very
beginning of the '.text.crt0' section. See the Section [Startup code] for
more details.


Symbols defined by the linker script
------------------------------------

The following symbols are defined by the linker script and used by the
base framework.

:'_prog_img_beg, _prog_img_data, _prog_img_end':
  Those symbols mark the start of the "text" part, the start of the "data"
  part (the end of the "text" part), and the end of the "bss" part.
  They are used by core to exclude those virtual memory ranges from
  the core's virtual-memory allocator (core-region allocator).

:'_parent_cap, _parent_cap_thread_id, _parent_cap_local_name':
  Those symbols are located at the beginning of the "data" part.
  During the ELF loading of a new component, the parent writes
  information about the parent capability to this location (the start
  of the first read-writable ELF segment). See the corresponding code
  in the '_setup_elf' function in _base/src/base/process/process.cc_.
  The use of the information depends on the used base platforms. E.g.,
  on a platform where a capability is represented by a tuple of a global
  thread ID and an object ID such as OKL4 and L4ka::Pistachio, those
  information are taken as verbatim values. On platforms that fully
  support capability-based security without the use of any form of
  a global name to represent a capability, the information are unused.
  Here, the parent capability is represented with the same known
  local name in all components.

Even though the linker scripts are used across all base platforms, they
contain a few platform-specific supplements that are needed to support
the respective kernel ABIs. For example, the definition of the symbol
'__l4sys_invoke_indirect' is needed only on the Fiasco.OC platform and
is unused on the other base platforms. Please refer to the comments
in the linker script for further explanations.


Startup code
~~~~~~~~~~~~

The execution the initial thread of a new component starts at the ELF
entry point, which corresponds to the '_start' function. This is an
assembly function defined in _repos/base/platform/<arch>/crt0.s_ where
_<arch>_ is the CPU architecture (x86_32, x86_64, or ARM).


Assembly startup code
---------------------

The assembly startup code is position-independent code (PIC).
Because the Genode base libraries are linked against both statically-linked
and dynamically linked executables, they have to be compiled as PIC code.
To be consistent with the base libraries, the startup code needs to be
position-independent, too.

The code performs the following steps:

# Saving the initial state of certain CPU registers. Depending on the
  used kernel, these registers are used to pass information from the
  kernel to the core component. More details about this information
  are provided by Section [Bootstrapping and allocator setup]. The
  initial register values are saved in global variables named
  '_initial_<register>'. The global variables are located in the BSS
  segment. Note that those variables are used solely by core.

# Setting up the initial stack. Before the assembly code can call any
  higher-level C function, the stack pointer must be initialized to
  point the top of a valid stack. The initial stack is located in the
  BSS section and referred to by the symbol '_stack_high'. However,
  having a stack located within the BSS section is dangerous. If it
  overflows (e.g., by declaring large local variables, or recursive
  function calls), the stack would silently overwrite parts of the
  BSS and DATA sections located below the lower stack boundary. For prior
  known code, the stack can be dimensioned to a reasonable size. But
  for arbitrary application code, no assumption about
  the stack usage can be made. For this reason, the initial stack cannot
  be used for the entire lifetime of the component. Before any
  component-specific code is called, the stack needs to be relocated to
  another area of the virtual address space where the lower bound of the
  stack is guarded by empty pages. When using such a "real" stack, a
  stack overflow will produce a page fault, which can be handled or at least
  immediately detected. The initial stack is solely used to perform the
  steps needed to set up the real stack. Because those steps are the same for
  all components, the usage of the initial stack is bounded.

# Because the startup code used by statically linked components as well as
  the dynamic linker, the startup immediately calls the 'init_rtld' hook
  function.
  For regular components, the function does not do anything. The default
  implementation in _repos/base/src/platform/init_main_thread.cc_ is a weak
  function. The dynamic linker provides a non-weak implementation, which
  allows the linker to perform initial relocations of itself very early at
  the dynamic linker's startup.

# By calling the 'init_main_thread' function defined in
  _repos/base/src/platform/init_main_thread.cc_, the assembly code triggers
  the execution of all the steps needed for the creation of the real stack.
  The function is implemented in C++, uses the initial stack, and returns
  the address of the real stack.

# With the new stack pointer returned by 'init_main_thread', the assembly
  startup code is able to switch the stack pointer from initial stack to
  the real stack. From this point on, stack overflows cannot easily corrupt
  any data.

# With the real stack in place, the assembly code finally passes the control
  over to the C++ startup code provided by the '_main' function.


Initialization of the real stack along with the Genode environment
------------------------------------------------------------------

As mentioned above, the assembly code calls the 'init_main_thread' function
(located in _repos/base/src/platform/init_main_thread.cc_) for setting up the
real stack for the program. For placing a stack in dedicated portion of the
component's virtual address space, the function needs to overcome two
principle problems:

* It needs to obtain the backing store used for the stack, i.e., by
  allocating a dataspace from the component's RAM session as initialized
  by the parent.

* It needs to preserve a portion of its virtual address space for placing
  the stack and make the allocated memory visible within this portion.

In order to solve both problems, the function needs to obtain capabilities
for its RAM session and RM session from its parent. This comes down to
the need for performing RPC calls. First, for requesting the RM and RAM
session capabilities from the parent, and second, for invoking the session
capabilities to perform the RAM allocation and RM attach operations.

The RPC mechanism is based on C++. In particular, the mechanism supports
the propagation of C++ exceptions across RPC interfaces. Hence,
before being able to perform RPC calls, the program must initialize
the C++ runtime including the exception support.
The initialization of the C++ runtime, in turn, requires support for
dynamically allocating memory. Hence, a heap must be available.
This chain of dependencies ultimately results in the need to construct the
entire Genode environment as a side effect of initializing the real stack of
the program.

During the construction of the Genode environment (by calling
'Genode::env()'), the program requests its own RM, RAM, CPU, and PD session
from its parent, and initializes its heap ('env()->heap()').

With the environment constructed, the program is able to interact
with its own RM and RAM sessions and can principally realize the
initialization of the real stack. However, instead of merely allocating
a new RAM dataspace and attaching the dataspace to the RM session, a
so-called thread-context area is constructed. The thread-context area
is a secondary RM session that is attached as a dataspace to the component's
actual RM session (See the description of managed dataspaces in Section
[Address-space management (RM)]).
This way, virtual-memory allocations within the thread context area can be
managed manually. I.e., the spaces between the stack of different threads are
guaranteed to remain free from any attached dataspaces.
For constructing the thread-context area, a new RM session is created
(_repos/base/src/base/context_area.cc_).


Component-specific startup code
-------------------------------

With the Genode environment constructed and the initial stack switched
to a proper stack located in the thread-context area, the component-specific
startup code of the '_main' in _repos/base/src/platform/_main.cc_ can be
executed. This code is responsible for calling the global constructors
of the program before calling program's main function.

In accordance to the established signature of the 'main' function, taking
an argument list and an environment as arguments, the startup code supplies
these arguments but uses dummy default values. However, since the values
are taken from the global variables 'genode_argv', 'genode_argc', and
'genode_envp', a global constructor is able to override the default values.

The startup code in '_main.cc' is accompanied with support for _atexit_
handling. The atexit mechanism allows for the registration of handlers
to be called at the exit of the program. It is provided in the form of
a POSIX API by the C runtime. But it is also used by the compiler to
schedule the execution of the destructors of function-local static objects.
For the latter reason, the atexit mechanism cannot be merely provided
by the (optional) C runtime but must be supported by the base library.


Interaction of core with the underlying kernel
==============================================

The core is the root of the process tree. It is initialized and started
directly by the underlying kernel and has two purposes. First, it makes
the low-level physical resources of the machine available to other components
in the form of services. Those resources are the physical memory, processing
time, device resources, initial boot modules, and protection mechanisms (such
as the MMU, IOMMU, and virtualization extensions). It thereby
hides the peculiarities of the used kernel behind an API that is uniform
across all kernels supported by Genode. Core's second purpose is the
creation of the init component by using its own services and following the
steps described in Section [Component creation].

Even though core is executed in usermode, its role as the root of the
component tree makes it as critical as the kernel. It just happens to be
executed in a different processor mode. Whereas regular components solely
interact with the kernel when performing inter-component communication, core
interplays with the kernel more intensely. The following subsections go
into detail about this interplay.

The description tries to be general across the various kernels supported
by Genode. Note, however, that a particular kernel may deviate from the
general description.


Bootstrapping and allocator setup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At boot time, the kernel passes information about the physical resources and
the initial system state to core. Even though the mechanism and format of this
information varies from kernel to kernel, it generally covers the following
aspects:

* A list of free physical memory ranges
* A list of the physical memory locations of the boot modules along with their
  respective names
* The number of available CPUs
* All information needed to enable the initial thread to perform kernel
  operations


Core's allocators
-----------------

Core's kernel-specific platform initialization code (_core/platform.cc_)
uses this information to initialize the allocators used for keeping track
of physical resources. Those allocators are:

:RAM allocator: contains the ranges of the available physical memory
:I/O memory allocator: contains the physical address ranges of unused
  memory-mapped I/O resources. In general, all ranges not initially present in
  the RAM allocator are considered as I/O memory.
:I/O port allocator: contains the I/O ports on x86-based platforms that are
  currently not in use. This allocator is initialized with the entire
  I/O port range of 0 to 0xffff.
:IRQ allocator: contains the IRQs that are associated with IRQ sessions.
  This allocator is initialized with the entirety of the available IRQ
  numbers.
:Core-region allocator: contains the virtual memory regions of core that
  are not in use.

The RAM allocator and core-region allocators are subsumed in the so-called
core-memory allocator. In addition to aggregating both allocators, the
core-memory allocator allows for the allocation of core-local virtual-memory
regions that can be used for holding core-local objects. Each region
allocated from the core-memory allocator has to satisfy three conditions:

# It must be backed by a physical memory range (as allocated from the RAM
  allocator)
# It must have assigned a core-local virtual memory range (as allocated
  from the core-region allocator)
# The physical-memory range has the same size as the virtual-memory range
# The virtual memory ranges is mapped to the physical memory range using
  the MMU

Internally, the core-memory allocator maintains a so-called mapped-memory
allocator that contains ranges of ready-to-use core-local memory. If a new
allocation exceeds the available capacity, the core-memory allocator expands
its capacity by allocating a new physical memory region from the RAM
allocator, allocating a new core-virtual memory region from the core-region
allocator, and installing a mapping from the virtual region to the physical
region.

All memory allocators mentioned above are working at the granularity of
physical pages, i.e., 4 KiB.

The core-memory allocator is used as the backing store for core's heap.
It is expanded on demand but never shrunk.
This makes it unsuitable for allocating objects on the behalf of core clients
because allocations could not be reverted when closing the session.
It is solely used for dynamic memory allocations at the startup (e.g., the
memory needed for keeping the information about the boot modules),
and for keeping meta data for the allocators themselves.

; Boot modules
; ------------


Kernel-object creation
~~~~~~~~~~~~~~~~~~~~~~

Kernel objects are objects maintained within the kernel and used by the
kernel.
The exact notion of what a kernel object represents depends on the actual
kernel as the various kernels differ with respect to the abstractions they
provide.
Typical kernel objects are threads and protection domains.
Some kernels have kernel objects for memory mappings whereas others provide
page tables as kernel objects.
Whereas some kernels represent scheduling parameters as distinct kernel
objects, others subsume scheduling parameters to threads.
What all kernel objects have in common, though, is that they consume kernel
memory.
Most kernels of the L4 family preserve a fixed pool of memory for the
allocation of kernel objects.

If an arbitrary component was able to perform a kernel operation that triggers
the creation of a kernel object, the memory consumption of the kernel would
depend on the good behavior of all components. A misbehaving component may
thereby exhaust the kernel memory.

To counter this problem, on Genode, only core triggers the creation of kernel
objects and thereby guards the consumption of kernel memory. Note, however,
that not all kernels are able to prevent the creation of kernel objects
outside of core.


| TODO
; XXX zeroing out RAM dataspaces
; XXX keeping core's virtual memory void of non-core memory objects
;    even though core has the authority of all memory, it maps memory the
;    physical pages that it needs to access
; XXX page-fault handling
;    pager object
;    deferred page fault resolution


IOMMU support
=============

As discussed in Section [Direct memory access (DMA) transactions], misbehaving
device driver may exploit DMA transactions to circumvent their component
boundary. When executing Genode on the NOVA microhypervisor, however,
bus-master DMA is subjected to the IOMMU.

NOVAs interface to the IOMMU is quite elegant. The kernel simply
applies a subset of the (MMU) address space of a process (aka protection domain
in NOVA speak) to the (IOMMU) address space of a device. So the device's
address space can be managed in the same way as we normally manage the address
space of a process. The only missing link is the assignment of device address
spaces to process address spaces. This link is provided by the dedicated system
call "assign_pci" that takes a process identifier and a device identifier as
arguments. Of course, both arguments must be subjected to a security policy.
Otherwise, any process could assign any device to any other process. To enforce
security, the process identifier is a capability to the respective protection
domain and the device identifier is a virtual address where the extended PCI
configuration space of the device is mapped in the specified protection domain.
Only if a user-level device driver got access to the extended PCI configuration
space of the device, it is able to get the assignment in place.

To make NOVA's IOMMU support available to Genode components,
the ACPI driver has the ability to hand out the extended PCI configuration
space of a device, and a NOVA-specific extension ('assign_pci') to the PD session
interface can be used to associate a PCI device with a protection domain.

; XXX [image img/iommu_aware 63%]
;   NOVAs management of the IOMMU address spaces facilities the use of
;   driver-local virtual addresses as DMA addresses.

Even though these mechanisms combined principally
suffice to let drivers operate with the IOMMU enabled, in practice, the
situation is a bit more complicated. Because NOVA uses the same
virtual-to-physical mappings for the device as it uses for the process, the DMA
addresses the driver needs to supply to the device must be virtual addresses
rather than physical addresses. Consequently, to be able to make a device
driver usable on systems without IOMMU as well as on systems with IOMMU, the
driver needs to be IOMMU-aware and distinguish both cases. This is an
unfortunate consequence of the otherwise elegant mechanism provided by NOVA. To
relieve the device drivers from caring about both cases, Genode decouples
the virtual address space of the device from the virtual address space of the
driver. The former address space is represented by a Genode process called
_device PD_. Its sole purpose
is to hold mappings of DMA buffers that are accessible by the associated
device. By using one-to-one physical-to-virtual mappings for those buffers
within the device PD, each device PD contains a subset of the physical address
space. The ACPI driver performs the assignment of device PDs to PCI
devices. If a device driver intends to use DMA, it allocates a new DMA buffer
for a specific PCI device at the ACPI driver.
The ACPI driver responds to such a request by allocating a RAM dataspace at core,
attaching it to the device PD using the dataspace's physical address as virtual
address, and handing out the dataspace capability to the client. If the driver
requests the physical address of the dataspace, the returned address will be a
valid virtual address in the associated device PD.
From this design follows that a device driver must allocate DMA buffers at the
ACPI server (specifying the PCI device the buffer is intended for) instead of
using core's RAM service to allocate buffers anonymously. Note that the
current implementation of the ACPI server assigns all PCI devices to only one
device PD.

; XXX [image img/iommu_agnostic 80%]
;   By modelling a device address space as a dedicated process (device PD),
;   the traditional way of programming DMA transactions can be maintained,
;   even with the IOMMU enabled.


Capability mechanism in depth
=============================

| TODO

; Capability representation as C++ object
; Marshalling of capabilities as RPC arguments
; Life-time management


Execution on bare hardware (base-hw)
====================================

| TODO
; * Rationale for the kernel design
; * Generally available system calls
; * Syscalls exclusive to core


Execution on the NOVA microhypervisor (base-nova)
=================================================

| TODO
; * Special linker script at base-nova/src/platform/roottask.ld - why?
; * Link to NOVA spec
; * Relationship of NOVA kernel objects to core sessions
; * Capability protection
; * Genode specific modifications of the NOVA kernel


Dynamic linker
==============

| TODO

; XXX concept of ld.lib.so
; * start ld.lib.so as regular binary, execute generic startup code
; * requesting the binary
; * lazy relocation
; * managed dataspace for keeping shared libraries (why)
; * lookup main function as present in the dynamic binary


; XXX further possible topics
; * Locking
; * C++ runtime
; * Booting


