Under the hood
##############

This chapter gives insights into the inner functioning of the Genode OS
framework. In particular, it explains how the concepts explained in Chapter
[Architecture] are realized on different kernels and hardware platforms.


Component-local startup code and linker scripts
===============================================

All Genode components including core rely on the same startup code, which
is roughly outlined at the end of Section [Component creation]. This
section revisits the steps in more detail and refers to the corresponding
points in the source code. Furthermore, it provides background information
about the linkage of components, which is closely related to the startup
code.


Linker scripts
~~~~~~~~~~~~~~

Under the hood, the Genode build system uses three different linker scripts
located at _repos/base/src/platform/_:

:genode.ld: used for statically linked components, including
  the core component,

:genode_dyn.ld: used for dynamically linked components, i.e., components
  that are linked against at least one shared library,

:genode_rel.ld: used for shared libraries.

Additionally, there exists a special linker script for the dynamic linker
(Section [Dynamic linker]).

Each program image generated by the linker generally consists of three parts,
which appear consecutively in the component's virtual memory.

# A read-only "text" part contains sections for code, read-only
  data, and the list of global constructors and destructors.

  The startup code is placed in a dedicated section '.text.crt0', which
  appears right at the start of the segment. Thereby the link address of
  the component is known to correspond with the ELF entrypoint (the first
  instruction of the assembly startup code).
  This is useful when converting the ELF image of the base-hw version of
  core into a raw binary. Such a raw binary can be loaded directly into
  the memory of the target platform without the need for an ELF loader.

  The mechanisms for constructing the list of constructors and destructors
  differs between the CPU architecture and are defined by the architecture's
  ABI. On x86, the lists are represented by '.ctors.*' and '.dtors.*'.
  On ARM, the information about global constructors is represented by
  '.init_array' and there is no visible information about global destructors.

# A read-writable "data" part that is pre-populated with data.

# A read-writable "bss" part that is not physically present in the binary but
  known to be zero-initialized when the ELF image is loaded.

The link address is not defined in the linker script but specified as
linker argument. The default link address is specified in a platform-specific
spec file, e.g., _repos/base-nova/mk/spec-nova.mk_ for the NOVA platform.
Components that need to organize their virtual address space in a special
way (e.g., a virtual machine monitor that co-locates the guest-physical
address space with its virtual address space) may specify link addresses
that differ from the default by overriding the LD_TEXT_ADDR value.


ELF entry point
---------------

As defined at the start of the linker script via the ENTRY directive, the
ELF entrypoint is the function '_start'. This function is located at the very
beginning of the '.text.crt0' section. See the Section [Startup code] for
more details.


Symbols defined by the linker script
------------------------------------

The following symbols are defined by the linker script and used by the
base framework.

:'_prog_img_beg, _prog_img_data, _prog_img_end':
  Those symbols mark the start of the "text" part, the start of the "data"
  part (the end of the "text" part), and the end of the "bss" part.
  They are used by core to exclude those virtual memory ranges from
  the core's virtual-memory allocator (core-region allocator).

:'_parent_cap, _parent_cap_thread_id, _parent_cap_local_name':
  Those symbols are located at the beginning of the "data" part.
  During the ELF loading of a new component, the parent writes
  information about the parent capability to this location (the start
  of the first read-writable ELF segment). See the corresponding code
  in the '_setup_elf' function in _base/src/base/process/process.cc_.
  The use of the information depends on the used base platforms. E.g.,
  on a platform where a capability is represented by a tuple of a global
  thread ID and an object ID such as OKL4 and L4ka::Pistachio, those
  information are taken as verbatim values. On platforms that fully
  support capability-based security without the use of any form of
  a global name to represent a capability, the information are unused.
  Here, the parent capability is represented with the same known
  local name in all components.

Even though the linker scripts are used across all base platforms, they
contain a few platform-specific supplements that are needed to support
the respective kernel ABIs. For example, the definition of the symbol
'__l4sys_invoke_indirect' is needed only on the Fiasco.OC platform and
is unused on the other base platforms. Please refer to the comments
in the linker script for further explanations.


Startup code
~~~~~~~~~~~~

The execution the initial thread of a new component starts at the ELF
entry point, which corresponds to the '_start' function. This is an
assembly function defined in _repos/base/platform/<arch>/crt0.s_ where
_<arch>_ is the CPU architecture (x86_32, x86_64, or ARM).


Assembly startup code
---------------------

The assembly startup code is position-independent code (PIC).
Because the Genode base libraries are linked against both statically-linked
and dynamically linked executables, they have to be compiled as PIC code.
To be consistent with the base libraries, the startup code needs to be
position-independent, too.

The code performs the following steps:

# Saving the initial state of certain CPU registers. Depending on the
  used kernel, these registers are used to pass information from the
  kernel to the core component. More details about this information
  are provided by Section [Bootstrapping and allocator setup]. The
  initial register values are saved in global variables named
  '_initial_<register>'. The global variables are located in the BSS
  segment. Note that those variables are used solely by core.

# Setting up the initial stack. Before the assembly code can call any
  higher-level C function, the stack pointer must be initialized to
  point the top of a valid stack. The initial stack is located in the
  BSS section and referred to by the symbol '_stack_high'. However,
  having a stack located within the BSS section is dangerous. If it
  overflows (e.g., by declaring large local variables, or recursive
  function calls), the stack would silently overwrite parts of the
  BSS and DATA sections located below the lower stack boundary. For prior
  known code, the stack can be dimensioned to a reasonable size. But
  for arbitrary application code, no assumption about
  the stack usage can be made. For this reason, the initial stack cannot
  be used for the entire lifetime of the component. Before any
  component-specific code is called, the stack needs to be relocated to
  another area of the virtual address space where the lower bound of the
  stack is guarded by empty pages. When using such a "real" stack, a
  stack overflow will produce a page fault, which can be handled or at least
  immediately detected. The initial stack is solely used to perform the
  steps needed to set up the real stack. Because those steps are the same for
  all components, the usage of the initial stack is bounded.

# Because the startup code used by statically linked components as well as
  the dynamic linker, the startup immediately calls the 'init_rtld' hook
  function.
  For regular components, the function does not do anything. The default
  implementation in _repos/base/src/platform/init_main_thread.cc_ is a weak
  function. The dynamic linker provides a non-weak implementation, which
  allows the linker to perform initial relocations of itself very early at
  the dynamic linker's startup.

# By calling the 'init_main_thread' function defined in
  _repos/base/src/platform/init_main_thread.cc_, the assembly code triggers
  the execution of all the steps needed for the creation of the real stack.
  The function is implemented in C++, uses the initial stack, and returns
  the address of the real stack.

# With the new stack pointer returned by 'init_main_thread', the assembly
  startup code is able to switch the stack pointer from initial stack to
  the real stack. From this point on, stack overflows cannot easily corrupt
  any data.

# With the real stack in place, the assembly code finally passes the control
  over to the C++ startup code provided by the '_main' function.


Initialization of the real stack along with the Genode environment
------------------------------------------------------------------

As mentioned above, the assembly code calls the 'init_main_thread' function
(located in _repos/base/src/platform/init_main_thread.cc_) for setting up the
real stack for the program. For placing a stack in dedicated portion of the
component's virtual address space, the function needs to overcome two
principle problems:

* It needs to obtain the backing store used for the stack, i.e., by
  allocating a dataspace from the component's RAM session as initialized
  by the parent.

* It needs to preserve a portion of its virtual address space for placing
  the stack and make the allocated memory visible within this portion.

In order to solve both problems, the function needs to obtain capabilities
for its RAM session and RM session from its parent. This comes down to
the need for performing RPC calls. First, for requesting the RM and RAM
session capabilities from the parent, and second, for invoking the session
capabilities to perform the RAM allocation and RM attach operations.

The RPC mechanism is based on C++. In particular, the mechanism supports
the propagation of C++ exceptions across RPC interfaces. Hence,
before being able to perform RPC calls, the program must initialize
the C++ runtime including the exception support.
The initialization of the C++ runtime, in turn, requires support for
dynamically allocating memory. Hence, a heap must be available.
This chain of dependencies ultimately results in the need to construct the
entire Genode environment as a side effect of initializing the real stack of
the program.

During the construction of the Genode environment (by calling
'Genode::env()'), the program requests its own RM, RAM, CPU, and PD session
from its parent, and initializes its heap ('env()->heap()').

With the environment constructed, the program is able to interact
with its own RM and RAM sessions and can principally realize the
initialization of the real stack. However, instead of merely allocating
a new RAM dataspace and attaching the dataspace to the RM session, a
so-called thread-context area is constructed. The thread-context area
is a secondary RM session that is attached as a dataspace to the component's
actual RM session (See the description of managed dataspaces in Section
[Address-space management (RM)]).
This way, virtual-memory allocations within the thread context area can be
managed manually. I.e., the spaces between the stack of different threads are
guaranteed to remain free from any attached dataspaces.
For constructing the thread-context area, a new RM session is created
(_repos/base/src/base/context_area.cc_).


Component-specific startup code
-------------------------------

With the Genode environment constructed and the initial stack switched
to a proper stack located in the thread-context area, the component-specific
startup code of the '_main' in _repos/base/src/platform/_main.cc_ can be
executed. This code is responsible for calling the global constructors
of the program before calling program's main function.

In accordance to the established signature of the 'main' function, taking
an argument list and an environment as arguments, the startup code supplies
these arguments but uses dummy default values. However, since the values
are taken from the global variables 'genode_argv', 'genode_argc', and
'genode_envp', a global constructor is able to override the default values.

The startup code in '_main.cc' is accompanied with support for _atexit_
handling. The atexit mechanism allows for the registration of handlers
to be called at the exit of the program. It is provided in the form of
a POSIX API by the C runtime. But it is also used by the compiler to
schedule the execution of the destructors of function-local static objects.
For the latter reason, the atexit mechanism cannot be merely provided
by the (optional) C runtime but must be supported by the base library.


C++ runtime
===========

Genode is implemented in C++ and relies on all C++ features required to use
the language in its idiomatic way. This includes the use of exceptions
and runtime-type information.


Rationale behind using exceptions
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Compared to return-based error handling as prominently used in C programs, the
C++ exception mechanism much more complex. In particular it requires the use
of a C++ runtime library that is used as a back-end by the exception handling code
generated by the compiler. This library contains the functionality needed to
unwind the stack and a mechanism for obtaining runtime type
information (RTTI). The C++ runtime libraries that come with common tool
chains, in turn, rely on a C library for performing dynamic memory
allocations, string operations, and I/O operations. Consequently, C++ programs
that rely on exceptions and RTTI use to depend on a C library. For this
reason, the use of those C++ features is universally disregarded for low-level
operating-systems code that usually does not run in an environment where a
complete C library is available.

In principle, C++ can be used without exceptions and RTTI (by passing the
arguments '-fno-exceptions' and '-fno-rtti' to GCC). However, without
those features, it is hardly possible to use the language as designed.

For example, when the operator 'new' is used, it performs two steps:
Allocating the memory needed to hold the to-be-created object and calling
the constructor of the object with the return value of the allocation
as 'this' pointer. In the event that the memory allocation fails, the only
way for the allocator to propagate the out-of-memory condition is throwing an
exception. If such an exception is not thrown, the constructor would be
called with a null as 'this' pointer.

Another example is the handling of errors during the construction of an
object. The object construction may consist of several consecutive
steps such as the construction of base classes and aggregated objects.
If one of those steps fails, the construction of the overall object remains
incomplete. This condition must be propagated to the code that issued the
object construction. There are two principle approaches:

# The error condition can be kept as an attribute in the object. After
  constructing the object, the user of the object may detect the error
  condition by requesting the attribute value.
  However, this approach is plagued by the following problems.

  First, the failure of one step
  may cause subsequent steps to fail as well. In the worst case, if the
  failed step initializes a pointer that is passed to subsequent
  steps, the subsequent steps may use an uninitialized pointer. Consequently,
  the error condition must eventually be propagated to subsequent steps,
  which, in turn, need to be implemented in a defensive way.

  Second, if the construction failed, the object exists but it is inconsistent.
  In the worst case, if the user of the object misses to check for the
  successful construction, it will perform operations on an inconsistent
  object. But even in the good case, where the user detects the
  incomplete construction and decides to immediately destruct the object, the
  destruction is error prone.
  The already performed steps may have had side effects such as resource
  allocations. So it is important to revert all the successful steps by
  invoking their respective destructors. However, when destructing the
  object, the destructors of the incomplete steps are also called.
  Consequently, such destructors need to be implemented in a defensive
  manner to accommodate this situation.

  Third, objects cannot have references that depend on potentially failing
  construction steps. In contrast to a pointer that may be marked as
  uninitialized by being a null pointer, a reference is, by definition,
  initialized once it exists. Consequently, the result of such a step can
  never be passed as reference to subsequent steps. Pointers must be used.

  Fourth, the mere existence of incompletely constructed
  objects introduces many variants of possible failures that need
  to be considered in the code. There may be many different stages of
  incompleteness. Because of the third problem,
  every time a construction step takes the result of previous step as
  argument, it explicitly has to consider the error case.
  This, in turn, tremendously inflates the test space of the code.

  Furthermore, there needs to be a convention of how the completion of an
  object is indicated. All programmers have to learn and follow the convention.

# The error condition triggers an exception. Thereby, the object construction
  immediately stops at the erroneous step. Subsequent steps are not
  executed at all. Furthermore, while unwinding the stack, the exception
  mechanism reverts all already completed steps by calling their respective
  destructors. Consequently, the construction of an object can be considered
  as a transaction. If it succeeds, the object is known to be completely
  constructed. If if fails, the object immediately ceases to exist.

Thanks to the transactional semantic of the second variant, the state space
for potential error conditions (and thereby the test space) remains small.
Also, the second variant facilitates the use of references as class members,
which can be safely passed as arguments to subsequent constructors. When
receiving such a reference as argument (as opposed to a pointer), no
validity checks are needed.
Consequently, by using exceptions, the robustness of object-oriented code
(i.e., code that relies on C++ constructors) can be greatly improved over code
that avoids exceptions.


Bare-metal C++ runtime
~~~~~~~~~~~~~~~~~~~~~~

Acknowledging the rationale given in the previous sections, there is
still the problem of the complexity added by the exception mechanism.
For Genode, the complexity of the trusted computing base is a fundamental
metric. The C++ exception mechanism with its dependency to the C library
arguably adds significant complexity. The code complexity of a C
library exceeds the complexity of the fundamental components (such as the
kernel, core, and init) by an order of magnitude. Making the fundamental
components depend on such a C library would jeopardize Genode's most valuable
asset, which is its low complexity.

To enable the use of C++ exceptions and runtime type information but
avoid the incorporation of an entire C library into the trusted computing
base, Genode comes with a customized C++ runtime that does not depend on
a C library. The C++ runtime libraries are provided by the tool chain.
To build those libraries without a C library, a libc emulation header
(_tool/libgcc_libc_stub.h_) is used instead of the interface of a real
C library. The emulation header contains only those definitions and
declarations needed by the C++ runtime. The resulting libraries contain
references to (some of) the symbols present in the emulation header.
Those symbols are provided by Genode's C++ support code
(_repos/base/src/base/cxx_). The implementation of those functions is
specifically tied to the usage patterns of the C++ runtime.
Hence, most of the functions are mere dummies.

Unfortunately, the interface used by the C++ runtime does not reside
in a specific namespace but it is rather a subset of the POSIX API. Hence, when
linking a real C library to a Genode component, the symbols present in the
C library would collide with the symbols present in Genode's C++ support code.
For this reason, the C++ runtime (of the compiler) and Genode's C++
support code are wrapped in a single library (_repos/base/lib/mk/cxx.mk_) in
a way that all POSIX functions remain hidden. All the references of the
C++ runtime are resolved by the C++ support code, both wrapped in the cxx
library. To the outside, the cxx library solely exports the CXA ABI as
required by the compiler.


Interaction of core with the underlying kernel
==============================================

The core is the root of the process tree. It is initialized and started
directly by the underlying kernel and has two purposes. First, it makes
the low-level physical resources of the machine available to other components
in the form of services. Those resources are the physical memory, processing
time, device resources, initial boot modules, and protection mechanisms (such
as the MMU, IOMMU, and virtualization extensions). It thereby
hides the peculiarities of the used kernel behind an API that is uniform
across all kernels supported by Genode. Core's second purpose is the
creation of the init component by using its own services and following the
steps described in Section [Component creation].

Even though core is executed in usermode, its role as the root of the
component tree makes it as critical as the kernel. It just happens to be
executed in a different processor mode. Whereas regular components solely
interact with the kernel when performing inter-component communication, core
interplays with the kernel more intensely. The following subsections go
into detail about this interplay.

The description tries to be general across the various kernels supported
by Genode. Note, however, that a particular kernel may deviate from the
general description.


Bootstrapping and allocator setup
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

At boot time, the kernel passes information about the physical resources and
the initial system state to core. Even though the mechanism and format of this
information varies from kernel to kernel, it generally covers the following
aspects:

* A list of free physical memory ranges
* A list of the physical memory locations of the boot modules along with their
  respective names
* The number of available CPUs
* All information needed to enable the initial thread to perform kernel
  operations


Core's allocators
-----------------

Core's kernel-specific platform initialization code (_core/platform.cc_)
uses this information to initialize the allocators used for keeping track
of physical resources. Those allocators are:

:RAM allocator: contains the ranges of the available physical memory
:I/O memory allocator: contains the physical address ranges of unused
  memory-mapped I/O resources. In general, all ranges not initially present in
  the RAM allocator are considered as I/O memory.
:I/O port allocator: contains the I/O ports on x86-based platforms that are
  currently not in use. This allocator is initialized with the entire
  I/O port range of 0 to 0xffff.
:IRQ allocator: contains the IRQs that are associated with IRQ sessions.
  This allocator is initialized with the entirety of the available IRQ
  numbers.
:Core-region allocator: contains the virtual memory regions of core that
  are not in use.

The RAM allocator and core-region allocators are subsumed in the so-called
core-memory allocator. In addition to aggregating both allocators, the
core-memory allocator allows for the allocation of core-local virtual-memory
regions that can be used for holding core-local objects. Each region
allocated from the core-memory allocator has to satisfy three conditions:

# It must be backed by a physical memory range (as allocated from the RAM
  allocator)
# It must have assigned a core-local virtual memory range (as allocated
  from the core-region allocator)
# The physical-memory range has the same size as the virtual-memory range
# The virtual memory ranges is mapped to the physical memory range using
  the MMU

Internally, the core-memory allocator maintains a so-called mapped-memory
allocator that contains ranges of ready-to-use core-local memory. If a new
allocation exceeds the available capacity, the core-memory allocator expands
its capacity by allocating a new physical memory region from the RAM
allocator, allocating a new core-virtual memory region from the core-region
allocator, and installing a mapping from the virtual region to the physical
region.

All memory allocators mentioned above are working at the granularity of
physical pages, i.e., 4 KiB.

The core-memory allocator is used as the backing store for core's heap.
It is expanded on demand but never shrunk.
This makes it unsuitable for allocating objects on the behalf of core clients
because allocations could not be reverted when closing the session.
It is solely used for dynamic memory allocations at the startup (e.g., the
memory needed for keeping the information about the boot modules),
and for keeping meta data for the allocators themselves.

; Boot modules
; ------------


Kernel-object creation
~~~~~~~~~~~~~~~~~~~~~~

Kernel objects are objects maintained within the kernel and used by the
kernel.
The exact notion of what a kernel object represents depends on the actual
kernel as the various kernels differ with respect to the abstractions they
provide.
Typical kernel objects are threads and protection domains.
Some kernels have kernel objects for memory mappings whereas others provide
page tables as kernel objects.
Whereas some kernels represent scheduling parameters as distinct kernel
objects, others subsume scheduling parameters to threads.
What all kernel objects have in common, though, is that they consume kernel
memory.
Most kernels of the L4 family preserve a fixed pool of memory for the
allocation of kernel objects.

If an arbitrary component was able to perform a kernel operation that triggers
the creation of a kernel object, the memory consumption of the kernel would
depend on the good behavior of all components. A misbehaving component may
thereby exhaust the kernel memory.

To counter this problem, on Genode, only core triggers the creation of kernel
objects and thereby guards the consumption of kernel memory. Note, however,
that not all kernels are able to prevent the creation of kernel objects
outside of core.


Page-fault handling
~~~~~~~~~~~~~~~~~~~

Each time a thread within the Genode system triggers a page fault, the kernel
reflects the page fault along with the fault information as a message to the
user-level page-fault handler residing in core.
The fault information comprises the identity and instruction pointer of the
faulted thread, the page-fault address, and the fault type (read, write,
execute).
The page-fault handler represents each thread as a so-called _pager object_.
For handling the page fault, it first looks up the pager
object that belongs to the faulting thread's identity,
analogously to how an RPC entrypoint looks up the RPC object for an incoming
RPC request.
Given the pager object, the fault is handled by calling the 'pager' function
with the fault information as argument. This function is implemented by
the so-called 'Rm_client' (_repos/base/src/core/rm_session_component.cc_),
which represents the association of the pager object
with its virtual address space (RM session). Given the context
information about the RM session of the thread, the 'pager' function
looks up the region within the RM session, on which the page fault occurred.
The lookup results in one of the following three cases:

:Region is populated with a dataspace:
  If a dataspace is attached at the fault address, the backing store of the
  dataspace is determined.
  Depending on the kernel, the backing store
  may be a physical page, a core-local page, or another reference to a physical
  memory page.
  The pager function then installs a memory mapping from the virtual page where
  the fault occurred to the corresponding part of the backing store.

:Region is populated with a managed dataspace:
  If the fault occurred within a region where a managed dataspace is
  attached, the fault handling is forwarded to the RM session that
  represents the managed dataspace.

:Region is empty:
  If no dataspace could be found at the fault address, the fault cannot
  be resolved. In this case, core submits an RM-fault signal to the RM
  session where the fault occurred. This way, the RM-session client has
  the chance to detect and possibly respond to the fault. Once the signal
  handler receives a fault signal, it is able to query the fault address
  from the RM session.
  As a response to the fault, the RM-session client may attach a dataspace at
  this address.
  This attach operation, in turn, will prompt core to wake up the thread
  (or multiple threads) that faulted within the attached region.
  If no signal handler for RM faults is registered for the RM session,
  core prints a diagnostic message and blocks the faulting thread forever.

To optimize the TLB footprint and the use of kernel memory, the RM service
does not merely work at the granularity of memory pages but operates on
address ranges those size and alignment are arbitrary power-of-two values (at
least as large as the size of the smallest physical page).
So the source and destinations of memory mappings may span many pages. This
way, depending on the kernel and the architecture, multiple pages may be
mapped at once, or large page-table mappings can be used.

| TODO
|
| XXX zeroing out RAM dataspaces, keeping core's virtual memory void of non-core memory objects
|


IOMMU support
=============

As discussed in Section [Direct memory access (DMA) transactions], misbehaving
device driver may exploit DMA transactions to circumvent their component
boundary. When executing Genode on the NOVA microhypervisor, however,
bus-master DMA is subjected to the IOMMU.

NOVAs interface to the IOMMU is quite elegant. The kernel simply
applies a subset of the (MMU) address space of a process (aka protection domain
in NOVA speak) to the (IOMMU) address space of a device. So the device's
address space can be managed in the same way as we normally manage the address
space of a process. The only missing link is the assignment of device address
spaces to process address spaces. This link is provided by the dedicated system
call "assign_pci" that takes a process identifier and a device identifier as
arguments. Of course, both arguments must be subjected to a security policy.
Otherwise, any process could assign any device to any other process. To enforce
security, the process identifier is a capability to the respective protection
domain and the device identifier is a virtual address where the extended PCI
configuration space of the device is mapped in the specified protection domain.
Only if a user-level device driver got access to the extended PCI configuration
space of the device, it is able to get the assignment in place.

To make NOVA's IOMMU support available to Genode components,
the ACPI driver has the ability to hand out the extended PCI configuration
space of a device, and a NOVA-specific extension ('assign_pci') to the PD session
interface can be used to associate a PCI device with a protection domain.

; XXX [image img/iommu_aware 63%]
;   NOVAs management of the IOMMU address spaces facilities the use of
;   driver-local virtual addresses as DMA addresses.

Even though these mechanisms combined principally
suffice to let drivers operate with the IOMMU enabled, in practice, the
situation is a bit more complicated. Because NOVA uses the same
virtual-to-physical mappings for the device as it uses for the process, the DMA
addresses the driver needs to supply to the device must be virtual addresses
rather than physical addresses. Consequently, to be able to make a device
driver usable on systems without IOMMU as well as on systems with IOMMU, the
driver needs to be IOMMU-aware and distinguish both cases. This is an
unfortunate consequence of the otherwise elegant mechanism provided by NOVA. To
relieve the device drivers from caring about both cases, Genode decouples
the virtual address space of the device from the virtual address space of the
driver. The former address space is represented by a Genode process called
_device PD_. Its sole purpose
is to hold mappings of DMA buffers that are accessible by the associated
device. By using one-to-one physical-to-virtual mappings for those buffers
within the device PD, each device PD contains a subset of the physical address
space. The ACPI driver performs the assignment of device PDs to PCI
devices. If a device driver intends to use DMA, it allocates a new DMA buffer
for a specific PCI device at the ACPI driver.
The ACPI driver responds to such a request by allocating a RAM dataspace at core,
attaching it to the device PD using the dataspace's physical address as virtual
address, and handing out the dataspace capability to the client. If the driver
requests the physical address of the dataspace, the returned address will be a
valid virtual address in the associated device PD.
From this design follows that a device driver must allocate DMA buffers at the
ACPI server (specifying the PCI device the buffer is intended for) instead of
using core's RAM service to allocate buffers anonymously. Note that the
current implementation of the ACPI server assigns all PCI devices to only one
device PD.

; XXX [image img/iommu_agnostic 80%]
;   By modelling a device address space as a dedicated process (device PD),
;   the traditional way of programming DMA transactions can be maintained,
;   even with the IOMMU enabled.


Capability mechanism in depth
=============================

| TODO

; Capability representation as C++ object
; Marshalling of capabilities as RPC arguments
; Life-time management


Dynamic linker
==============

| TODO

; XXX concept of ld.lib.so
; * start ld.lib.so as regular binary, execute generic startup code
; * requesting the binary
; * lazy relocation
; * managed dataspace for keeping shared libraries (why)
; * lookup main function as present in the dynamic binary


Execution on bare hardware (base-hw)
====================================

The code specific to the base-hw platform is located within the
_repos/base-hw/_ directory. In the following description, unless explicitly
stated otherwise, all paths are relative to this directory.

In contrast to classical L4 microkernels where Genode's core process runs as
user-level roottask on top of the kernel, base-hw executes Genode's core
directly on the hardware with no distinct kernel underneath. Core and kernel
are melted into one hybrid kernel/userland program. Only a few
code paths are executed in privileged mode but most code runs in user mode.
This design has several benefits. First, the kernel part becomes much simpler.
For example, there are no allocators needed in the kernel part because
allocators are managed by the user-level part of core. Second, base-hw
side-steps long-standing hard kernel-level problems, in particular the
management of kernel resources. For the allocation of kernel objects, the
hybrid core/kernel can
employ Genode's user-level resource trading concepts as described in Section
[Resource trading]. Finally and most
importantly, merging the kernel with roottask removes a lot of
redundancies between both programs. Traditionally, both kernel and roottask
performed the book keeping of physical-resource allocations and the existence
of kernel objects such as address spaces and threads. In base-hw, those data
structures exist only once. The complexity of the combined kernel/core is
significantly lower than the sum of the complexities of a traditional
self-sufficient kernel and a distinct roottask on top. This way, base-hw helps
to make Genode's TCB less complex.

The following subsections detail the problems that base-hw had to address
to become a self-sufficient base platform for Genode.


Bootstrapping of base-hw
~~~~~~~~~~~~~~~~~~~~~~~~

A Genode-based system consists of potentially many boot modules. But
boot loaders on ARM platforms usually merely support the loading of a
single system image. Hence, base-hw requires a concept for merging
boot modules together with the core/kernel into a single image.

| XXX how does the concept works?


Kernel entry and exit
~~~~~~~~~~~~~~~~~~~~~

The execution model of the kernel can be roughly characterized as a
single-stack kernel. In contrast to traditional L4 kernels that maintain one
kernel thread per user thread, the base-hw kernel is a mere state machine
that never blocks in the kernel. State transitions are triggered by
user-level threads that enter the kernel via a system call, by device
interrupts, or by a CPU exception. Once entered, the kernel applies the state
change depending on the event that caused the kernel entry, and leaves the
kernel to the user land. The transition between user and kernel mode depends
on the revision of the ARM architecture. For ARMv7, the corresponding code is
located at _src/core/spec/arm_v7/mode_transition.s_.


Interrupt handling and preemptive multi-threading
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

In order to respond to interrupts, base-hw has to contain a driver for
the interrupt controller. ARM-based SoCs greatly differ with respect
to the used interrupt controllers. The interrupt-controller driver for
a particular SoC can be found at _src/core/include/spec/<spec>/pic.h_
and the corresponding _src/core/spec/<spec>/pic.cc_ where _<spec>_
refers to a particular platform (e.g., imx53) or an IP block that is
is used across different platforms (e.g., arm_gic for ARM's generic
interrupt controller).
Each of the drivers implement the same interface. When building core,
the build system uses the build-spec mechanism explained in
Section [Build system] to incorporate the single driver needed for the
targeted SoC.

To support preemptive multi-threading, base-hw requires a hardware timer.
The timer is programmed with the timeslice length of the currently
executed thread. Once the programed timeout elapses, the timer device
generates an interrupt that is handled by the kernel. Similarly to
interrupt controllers, there exist a variety of different timer devices
on ARM-based SoCs. Therefore, base-hw contains different timer drivers.
The timer drivers are located at _src/core/include/spec/<spec>/timer.h_
where _<spec>_ refers to the timer variant.

The in-kernel handler of the timer interrupt invokes the thread scheduler
(_src/core/include/kernel/cpu_scheduler.h_).
The scheduler maintains a list of so-called scheduling contexts where each
context refers to a thread. Each time, the kernel is entered, the scheduler
is updated with the passed duration. When updated, it takes a scheduling
decision by making the next to-be-executed thread the head of the list.
At the kernel exit, the control is passed to the user-level thread that
corresponds to the head of the scheduler list.


Split kernel interface
~~~~~~~~~~~~~~~~~~~~~~

The system-call interface of the base-hw kernel is split in two parts.
One part is usable by all components and solely contains system calls for
inter-component communication and thread synchronization. The definition
of this interface is located at _include/kernel/interface.h_. The second
part is exposed only to core. It supplements the public interface with
operations for the creation, the management, and the destruction of kernel
objects.

The distinction between both parts of the kernel interface is enforced
by the function 'Thread::_call' in _src/core/kernel/thread.cc_.


Public part of the kernel interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Threads do not run independently but interact with each other via synchronous
inter-component communication as detailed in Section
[Inter-component communication]. Within base-hw, this mechanism is referred
to as IPC (for inter-process communication).
To allow threads to perform calls to other threads or to receive RPC requests,
the kernel interface is equipped with system calls for performing IPC
(_send_request_msg_, _await_request_msg_, _send_reply_msg_.
To keep the kernel as simple as possible, IPC is performed using so-called
user-level thread-control blocks (UTCB).
Each thread has a corresponding memory page that is always
mapped in the kernel. This UTCB page is used to carry IPC payload. The largely
simplified procedure of transferring a message is as follows. (In reality, the
state space is more complex because the receiver may not be in a blocking state
when the sender issues the message)

# The user-level sender marshals its payload into its UTCB and invokes the
  kernel,
# The kernel transfers the payload from the sender's UTCB to the receiver's
  UTCB and schedules the receiver,
# The receiver retrieves the incoming message from its UTCB.

Because all UTCBs are always mapped in the kernel, no page faults can occur
during the second step. This way, the flow of execution within the kernel
becomes predictable and always returns to the user land.

In addition to IPC, threads interact via the synchronization primitives
provided by the Genode API. To implement these portions of the API, the kernel
provides system calls for managing the execution control of threads
(_pause_current_thread_, _resume_local_thread_, _yield_thread_).

To support asynchronous notifications as described in Section
[Asynchronous notifications], the kernel provides system calls for the
submission and reception of signals (_await_signal_, _signal_pending_,
_submit_signal_, and _ack_signal_) as well as the life-time management
of signal contexts (_kill_signal_context_). In contrast to other
base platforms, Genode's signal API is directly supported by the kernel
so that the propagation of signals does not require any interaction with
core's SIGNAL service (Section [Asynchonous notifications (SIGNAL)]).
However, the creation of signal contexts is arbitrated by the SIGNAL service.
This way, the kernel objects needed for the signalling mechanisms are
accounted to the corresponding clients of the SIGNAL service.


Core-private part of the kernel interface
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The core-private part of the kernel interface allows the user-level part
of core to perform privileged operations. Note that even though the
kernel and core are executed in different CPU modes (privileged mode and
user mode), both parts share a single address space and ultimately trust
each other. The kernel is regarded a mere support library of core that
executes those functions that can only be executed in the privileged
CPU mode. In particular, the kernel does not perform any allocation.
Instead, the allocation of a kernel objects is performed as an interplay
of core and the kernel.

# Core allocates physical memory from its physical-memory allocator.
  Most kernel-object allocations are performed in the context of one
  of core's services. Hence, those allocations can be properly accounted
  to a session quota (Section [Resource trading]). This way, kernel objects
  allocated on behalf of core's clients are "paid for" by those clients.

# Core allocates virtual memory to make the allocated physical memory visible
  within core and the kernel.

# Core invokes the kernel to construct the kernel object at the location
  specified by core. This kernel invocation is actually a system call that
  enters the kernel via the kernel-entry path.

# The kernel initializes the kernel object at the virtual address specified
  by core and returns to core via the kernel-exit path.

The core-private kernel interface consists of the following operations:

* The creation and destruction of protection domains
  (_new_pd_ and _bin_pd_), invoked by the PD service
* The creation, manipulation, and destruction of threads
  (_new_thread_, _bin_thread_, _start_thread_, _resume_thread_,
  _access_thread_regs_, and _route_thread_event_), used by the CPU service
  and the core-specific back end of the 'Genode::Thread' API
* The creation and destruction of signal receivers and signal contexts
  (_new_signal_receiver_, _bin_signal_receiver_, _new_signal_context_, and
  _bin_signal_context_), invoked by the SIGNAL service


Scheduler of the base-hw kernel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

CPU scheduling in traditional L4 microkernels is based on static priorities.
The scheduler always picks the runnable thread with highest priority for
execution.
If multiple threads share one priority, the kernel schedules those threads
in a round-robin fashion.
Whereas being pretty fast and easy to implement, this scheme has disadvantages:
First, there is no way to prevent
high-prioritized threads from starving lower-prioritized ones. Second, CPU time
could not be granted to threads and passed between them by the means of quota.
To cope with these problems without much loss of performance, base-hw employs
a custom scheduler that deviates from the traditional approach.

The base-hw scheduler introduces the distinction between high-throughput-oriented
scheduling contexts - called _fills_ - and low-latency-oriented
scheduling contexts - called _claims_. Examples for typical fills would be
the processing of a compiler job or the rendering computations of a sophisticated
graphics program. They shall obtain as much CPU time as the system can spare
but there is no demand for a high responsiveness. In contrast, a good example
for the claim category would be a typical GUI-software stack covering the
control flow from user-input drivers through a chain of GUI components to the
drivers of the graphical output. Another example is a user-level device driver
that must quickly respond to sporadic interrupts but is otherwise untrusted.
The low latency of such components is a key factor for usability and
quality of service. Besides introducing the distinction between claim and fill
scheduling contexts, base-hw introduces the notion of a so-called
_super period_, which is a multiple of typical scheduling time slices, e.g.,
one second. The entire super period
corresponds to 100% of the CPU time of one CPU. Portions of it can be assigned
to scheduling contexts. A CPU quota thereby corresponds to a percentage of the
super period.

At the beginning of a super period, each claim has its full amount of assigned
CPU quota. The priority defines the absolute scheduling order within the super
period among those claims that are active and have quota left. As long as
there exist such claims, the scheduler stays in the claim mode and the quota
of the scheduled claims decreases. At the end of a super period, the quota of
all claims gets refreshed to the initial value. Every time the scheduler can't
find an active claim with CPU-quota left, it switches to the fill mode. Fills
are scheduled in a simple round-robin fashion with identical time slices. The
proceeding of the super period doesn't affect the scheduling order and
time-slices of this mode. The concept of quota and priority that is
implemented through the claim mode aligns nicely with Genode's way of
hierarchical resource management: Through CPU-sessions, each process becomes
able to assign portions of its CPU time and subranges of its priority band to
its children without knowing the global means of CPU time or priority.


Sparsely populated core address space
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Even though core has the authority over all physical memory, it has no
immediate access to the physical pages. Whenever core requires access to a
physical memory page, it first has to explicitly map the physical page into
its own virtual memory space. This way, the virtual address space of core
stays clean from any data of other components. Even in the presence of a bug
in core (e.g., a dangling pointer), information cannot accidentally leak
between different protection domains because the virtual memory of other
components is not visible to core.


Multi-processor support of base-hw
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

On uniprocessor systems, the base-hw kernel is single-threaded. Its
execution model corresponds to a mere state machine.
On SMP systems, it maintains one kernel thread and one scheduler per CPU core.
Access to kernel
objects gets fully serialized by one global spin lock that is acquired
when entering the kernel and released when leaving the kernel. This keeps the
use of multiple cores transparent to the kernel model, which greatly
simplifies the code compared to traditional L4 microkernels. Given
that the kernel is a simple state machine providing lightweight non-blocking
operations, there is little contention for the global kernel
lock. Even though this claim may not hold up when scaling to a large number of
cores, current ARM-based platforms can be accommodated well.


Cross-CPU inter-component communication
---------------------------------------

Regarding synchronous and asynchronous inter-processor communication - thanks
to the global kernel lock - the is no semantic difference to the uniprocessor
case. The only difference is that on a multiprocessor system, one processor may
change the schedule of another processor by unblocking one of its threads
(e.g., when RPC call is received by a server that resides on a different CPU
as the client).
This condition may rescind the current scheduling choice of the other processor.
To avoid lags in this case, the kernel lets the unaware target processor trap
into an inter-processor interrupt (IPI).
The targeted processor can respond to the IPI by taking the decision to
schedule the receiving thread.
As the IPI sender doesn't have to wait for an answer, the sending and
receiving CPUs remain largely decoupled.
There is no need for a complex IPI protocol between both.


TLB shootdown
-------------

With respect to the synchronization of core-local hardware, there are two
different situations to deal with. Some hardware components like most ARM
caches and branch predictors implement their own coherence protocol and thus
need adaption in terms of configuration only. Others, like the TLBs lack this
feature. When for instance a page table entry gets invalid, a TLB invalidation
of the affected entries must be done locally by each core. To signal the
necessity of TLB maintenance work, an IPI is sent to all other cores. If all
cores completed the cleaning, the thread that invoked the TLB invalidation
resumes its execution.

; Futher possible topics
; * TrustZone
; * kernel bootstrap


Execution on the NOVA microhypervisor (base-nova)
=================================================

NOVA is a so-called microhypervisor, denoting the combination of microkernel
and a virtualization platform (hypervisor). It is a high-performance
microkernel for the x86 architecture. In contrast to other microkernels,
it had been designed for hardware-based virtualization via user-level
virtual-machine monitors. In line with Genode's architecture, NOVA's kernel
interface is based on capability-based security. Hence, the kernel fully
supports the model of a Genode kernel as described in Section
[Capability-based security].

:NOVA website:

  [http://hypervisor.org]

:NOVA kernel-interface specification:

  [https://github.com/udosteinberg/NOVA/raw/master/doc/specification.pdf]


Integration of NOVA with Genode
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

The NOVA kernel is available via Genode's ports mechanism described in
Section [Ports of 3rd-party software]. The port description is located
at _repos/base-nova/ports/nova.port_.

Building the NOVA kernel
------------------------

Even though NOVA is a third-party kernel with a custom build system,
the kernel is built directly from the Genode build system. NOVA's build
system remains unused.

From within a Genode build directory configured for one of the nova_x86_32
or nova_x86_64 platforms, the kernel can be built via

! make kernel

The build description for the kernel is located at
_repos/base-nova/src/kernel/target.mk_.

System-call bindings
--------------------

NOVA is not accompanied with bindings to its kernel interface. There is
only a description of the kernel interface in the form of the kernel
specification available. For this reason, Genode maintains the kernel
bindings for NOVA within the Genode source tree. The bindings are located
at _repos/base-nova/include/_ in the subdirectories _nova/_, _32bit/nova/_,
and _64bit/nova/_.


Bootstrapping of a NOVA-based system
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

After finishing its initialization, the kernel starts the first boot module
(after the kernel) as root task. The root task is Genode's core. The virtual
address space of core contains the text and data segments of core, the UTCB
of the initial EC, and the hypervisor info page (HIP). Details about the
HIP are provided in Section 6 of the NOVA specification.

BSS section of core
-------------------

The kernel's ELF loader does not support the concept of a BSS segment. It
simply maps the physical pages of the core's text and data segments into
the virtual memory of the core but does not allocate any additional physical
pages for backing the BSS. For this reason, the NOVA version of core
does not use the _genode.ld_ linker script as described in Section
[Linker scripts] but the linker script located at
_repos/base-nova/src/platform/roottask.ld_. This version hosts the BSS section
within the data segment. Thereby, the BSS is physically present in the core
binary in the form of zero-initialized data.

Initial information provided by NOVA to core
--------------------------------------------

The kernel passes a pointer to the HIP to core as the initial value of the
ESP register. Genode's startup code saves this value in the global variable
'_initial_sp' (Section [Startup code]).


Log output on modern PC hardware
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Because transmitting information over the legacy comports does not require
complex device drivers, serial output over comports is still the predominant
way to output low-level system logs like kernel messages or the output of
core's LOG service.

Unfortunately, most modern PCs lack dedicated comports. This leaves two
options to obtain low-level system logs.

# The use of vendor-specific platform-management features such as Intel
  VPro / Intel Advanced Management Technology (AMT) or Intel Platform
  Management Interface (IPMI). These platform features are able to emulate a
  legacy comport and provide the serial output over the network.
  Unfortunately, those solutions are not uniform across different vendors,
  difficult to use, and tend to be unreliable.

# The use of a PCI card or an Express card that provides a physical comport.
  When using such a device, comport appears as PCI I/O resource.
  Because the device interface is compatible to the legacy comports,
  no special drivers are needed.

The latter option allows the retrieval of low-level system logs on hardware
that lacks special management features.
In contrast to the legacy comports, however, it has the minor disadvantage
that the location of the device's I/O resources is not prior known.
The I/O port range of the comport depends on the device-enumeration
procedure of the BIOS. To enable the kernel to output information
over this comport, the kernel must be configured with the I/O port range
as assigned by the BIOS on the specific machine. One version of the kernel
cannot simply be used across different machines.

The Bender chain boot loader
----------------------------

The alleviate the need to adapt the kernel configuration to the used comport
hardware, the bender chain boot loader can be used.

:Bender is part of the MORBO tools:

  [https://github.com/TUD-OS/morbo]

Instead of starting the NOVA hypervisor directly, the multi-boot-compliant
boot loader (such as GRUB) starts bender as the kernel. All remaining
boot modules including the real kernel have been already loaded into memory
by the original boot loader. Bender scans the PCI bus for a comport device.
If such a device is found (e.g., an Express card), it writes the information
about the device's I/O port range to a known offset within the BIOS data
area (BDA).

After the comport-device probing is finished, bender passes control to the
next boot module, which is the real kernel. The comport device driver of
the kernel does not use a hard-coded I/O port range for the comport but
looks up the comport location from the BDA.
The use of bender is optional. When not used, the BDA always contains the I/O
port range of the legacy comport 1.

The Genode source tree contains a pre-compiled binary of bender at
_tool/boot/bender_. This binary is automatically incorporated into boot images
for the NOVA base platform when the run tool (Section [Run tool]) is used.


Relation of NOVA's kernel objects to Genode's core services
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

For the terminology of NOVA's kernel objects, refer to the NOVA specification
mentioned in the introduction of
Section [Execution on the NOVA microhypervisor (base-nova)].
A brief glossary for the terminology used in the remainder of this section is
given in table [nova_terminology].

   NOVA term  |
  --------------------------------------------------
   PD         | Protection domain
   EC         | Execution context (thread)
   SC         | Scheduling context
   HIP        | Hupervisor information page
   IDC        | Inter-domain call (RPC call)

[table nova_terminology]
  Glossary of NOVA's terminology


NOVA capabilities are not Genode capabilities
---------------------------------------------

Both NOVA and Genode use the term "capability". However, the term has not
the same meaning in both contexts. A Genode capability refers to an RPC
object or a signal context. In the context of NOVA, a capability refers to
a NOVA kernel object. To avoid confusing both meanings of the term,
Genode refers to NOVA's term as "capability selector", or simply
"selector".


PD service
----------

A PD session corresponds to a NOVA PD.


CPU service
-----------

NOVA distinguishes so-called global ECs from local ECs. A global EC can
be equipped with CPU time by associating it with an SC. It can perform
IDC calls but it cannot receive IDC calls. In contrast to a global EC,
a local EC is able to receive IDC call but it has no CPU time. A local
EC is executed not before it is called by another EC.

A regular Genode thread is a global EC. A Genode entrypoint is a local EC.
Core distinguishes both cases based on the instruction-pointer (IP) argument
of the CPU session's start function. For a local EC, the IP is set to zero.


RAM and IO_MEM services
-----------------------

Core's RAM and IO_MEM allocators are initialized based on the information found in NOVA's
HIP.


ROM service
-----------

Core's ROM service provides all boot modules as ROM modules. Additionally,
NOVA's HIP is provided as a ROM module named "hypervisor_info_page".


CAP service
-----------

A Genode capability corresponds to a NOVA portal. Each NOVA portal has a
defined IP and an associated local EC (the Genode entrypoint). The invocation
of a Genode capability is an IDC call to a portal. A Genode capability is
delegated by passing its corresponding portal selector as IDC argument.


IRQ service
-----------

NOVA represents each interrupt as a semaphore. Within core, there is one
entrypoint per IRQ session. When 'wait_for_irq' is called, the called IRQ
entrypoint blocks on its corresponding IRQ semphore. In the kernel, this
semaphore-down operation implicitly unmasks the interrupt at the CPU.

When the interrupt occurs, the kernel masks the interrupt at the CPU and
performs the semphore-up operation on the IRQ's semaphore. Thereby, it wakes
up the IRQ entrypoint, which replies to the 'wait_for_irq' RPC call.


RM service
----------

The RM service is used for the page-fault handling as explained in Section
[Page-fault handling on NOVA]. Each memory mapping installed in a component
implicitly triggers the allocation of a node in the kernel's mapping
database.


Page-fault handing on NOVA
~~~~~~~~~~~~~~~~~~~~~~~~~~

On NOVA, each EC has a defined range of portal selectors that are traversed
in the event of an exception. For each type of exception there is dedicated
portal. The page-fault portal of a Genode thread is defined at the creation
time of the thread and points to a dedicated pager EC within core.
Hence, for each Genode thread, there exist two ECs. One in the PD where
the thread executes and the pager EC in core.


The operation of pager ECs
--------------------------

When an EC triggers a page fault, the faulting EC implicitly performs an
IDC call to its pager. The IDC message contains the fault information.
On NOVA, there is a one-to-one relationship between a pager EC and Genode's
pager object. For resolving the page fault, core follows the procedure
described in [Page-fault handling]. If the lookup for a dataspace within
the faulter's RM session succeeds, core establishes
a memory mapping into the EC's PD by sending a so-called map item as reply to
the page fault message. In the case where the region lookup
within the thread's corresponding RM session fails, the pager EC blocks
on a semaphore. Because the page-fault message remains unanswered, the
faulting thread is effectively put on halt.
In the event that the RM fault is resolved by an RM client
as described in the paragraph "Region is empty", the blocking on the
semaphore gets released and the pager EC is able to reply to the original
page-fault message. However, the reply does not immediately establish a memory
mapping. Instead, the faulter will immediately trigger another fault at the
same address. This time, however, the region lookup succeeds.


Mapping database
----------------

NOVA tracks memory mappings in a data structure called _mapping database_
and has the notion of the delegation of memory mappings (rather than the
delegation of memory access). Memory access can be delegated only if the
originator of the mapping has a mapping. Core is the only exception because
it can establish mappings originating from the physical memory space.
Because mappings can be delegated transitively between PDs, the mapping
database is a tree where each node denotes the delegation of a mapping.
The tree is maintained in order to enable the kernel to revoke the authority.
When a mapping is revoked, the kernel implicitly revokes all transitive
mappings that originated in the revoked node.

Because of this design, core needs to maintain a core-local memory mapping for
each memory mapping established outside of core.
This mapping is solely needed to revoke the memory mapping later on, for
example, when a dataspace is detached from an RM session.
The kernel's revoke operation takes the core-local address as argument and
revokes all mappings originating from this mapping node.


Genode-specific modifications of the NOVA kernel
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

NOVA is not fit to be used as Genode base platform as is. This section
compiles the modifications that were needed to meet the functional requirements of
the framework. All modifications are maintained at the following
repository:

:Genode's version of NOVA:

  [https://github.com/alex-ab/NOVA.git]

The repository contains a separate branch for each version of NOVA that had
been used for Genode. When preparing the NOVA port using the port description
at _repos/base-nova/ports/nova.port_, the NOVA branch that matches the used
Genode version is checked out automatically. The port description refers to
a specific commit ID. The commit history of each branch within the NOVA
repository above corresponds to the history of the original NOVA kernel
followed by a series of Genode-specific commits. Each time, NOVA is updated,
all Genode-specifications are rebased on the history of the new NOVA version.
This way, the differences between the original NOVA kernel and the Genode
version remain clearly documented. The Genode-specific modifications solve the
following problems:

:Destruction of kernel objects:

  NOVA does not support the destruction of kernel objects. E.e., PDs and
  ECs can be created but not destroyed. With Genode being a dynamic system,
  kernel-object destruction is a mandatory feature.

:Inter-processor IDC:

  On NOVA, only local ECs can receive IDC calls. Furthermore each local EC
  is bound to a particular CPU (hence the name "local EC"). Consequently,
  synchronous inter-component communication via IDC calls is possible only
  between ECs that both resided on the same CPU but can never cross CPU
  boundaries. Unfortunately, IDC is the only mechanism for the delegation
  of capabilities. Consequently, authority cannot be delegated between
  subsystems that reside on different CPUs. For Genode, this scheme is
  too rigid.

  Hence, the Genode version of NOVA introduces inter-CPU calls. When calling
  an EC on another CPU, the kernel creates a temporary EC and SC on the
  targeted CPU as a representative of the caller. The calling EC is blocked.
  The temporary EC uses the same UTCB as the calling EC. Therebe, the
  original IDC message is effectively transferred from one CPU to the other.
  The temporary EC then perform a local IDC to the destination EC using
  NOVA's existing IDC mechanism. Once the temporary EC receives the reply
  (with the reply message contained in the caller's UTCB), the kernel
  destroys the temporary EC and SC and unblocks the caller EC.

:Support for priority-inheriting spinlocks:

  Genode's lock mechanism relies on a yielding spinlock for protecting the
  lock meta data. On most base platform, there exists the invariant that
  all threads of one component share the same CPU priority. So priority
  inversion within a component cannot occur. NOVA breaks this invariant
  because the scheduling parameters (SC) are passed along IDC call chains.
  Consequently, when a client calls a server, the SCs of both client
  and server reside within the server. These SCs may have different
  priorities. The use of a naive spinlock for synchronization will produce
  priority inversion problems. The kernel has been extended with the
  mechanisms needed to support the implementation of
  priority-inheriting spinlock in the userland.

:Combination of capability delegation and translation:

  As described in
  Section [Capability delegation through capability invocation],
  there are two cases when a capability is specified as an RPC argument.
  The callee may already have a capability referring to the specified
  capability. In this case, the callee expects to receive the corresponding
  local name of the object identity. In the other case, when the callee
  does not yet have a capability for the object identity, it obtains a new
  local name that refers to the delegated capability.

  NOVA does not support this mechanism per se.
  When specifying a capability selector as map item for an IDC call,
  the caller can specify whether a new mapping should be created or
  the translation of the local names should be performed by the kernel.
  However, in the general case, this question is not decidable by the caller.
  Hence, NOVA had to be changed to take the decision depending on the
  existence of a valid translation for specified capability selector.


Known limitations of NOVA
~~~~~~~~~~~~~~~~~~~~~~~~~

This section summarizes the known limitations of NOVA and the NOVA version of
core.

:Fixed amount of kernel memory:

  NOVA allocates kernel objects out of a memory pool of a fixed size. The pool
  is dimensioned in the kernel's linker script
  _nova/src/hypervisor.ld_ (at the symbol '_mempool_f').
  The existence of a fixed pool implies that any component that is able to
  trigger allocations in the kernel is able to indirectly consume kernel
  resources. A misbehaving component in possession of its own PD capability
  selector may even forcefully trigger the
  exhaustion of the entire pool and thereby make the kernel unavailable. I.e.,
  the kernel panics when running out of memory. The kernel provides no
  mechanism to mitigate such an resource-exhaustion-based denial-of-service
  attach.

  On Genode, only core explicitly allocates kernel objects, which relieves
  the problem but does not solve it. In order to create a kernel
  object, a PD capability selector must be specified to the respective
  system call. Since PD capability selectors are never delegated to the
  outside of core, kernel objects cannot be directly created by arbitrary
  components. The creation of kernel objects is rather a side effect of the
  use of core's services. Thereby, core is principally in the position to
  restrict the use of kernel memory per client. However, such an accounting
  for kernel memory is not performed by the NOVA version of core.

  In addition to the explicit creation of kernel objects, kernel memory is
  implicitly allocated when mapping nodes are inserted into the kernel's
  mapping database. Thereby, kernel memory is consumed as a side effect of IDC
  calls that carry map items. Since ECs of the same PD can perform IDC calls
  between one another, the allocation of mapping nodes can be artificially
  stressed by delegating a large number of mappings within the same PD via
  successive IDC calls.

  Therefore, components are principally able to launch denial-of-service
  attacks on the kernel. In the event of an exhaustion of kernel memory,
  the kernel stops the system. Hence, even though the lack of proper
  management of kernel memory is an availability risk, it cannot be
  exploited as a covert storage channel.

:The maximum number of threads is limited by core's thread-context area:

  NOVA's page-fault handling protocol works completely synchronously. When a
  page fault occurs, the faulting EC enters its page-fault portal and thereby
  activates the corresponding pager EC in core. If the pager's lookup for a
  matching dataspace within the faulter's RM session succeeds, the page fault
  is resolved by delegating a memory mapping as the reply to the page-fault
  IDC call. However, if a page fault occurs on a managed dataspace, the pager
  cannot resolve it immediately. The resolution must be delayed until the RM
  fault handler (outside of core) responds to the RM fault signal. In order to
  be able to serve page faults of other threads in the meantime, each thread
  has its dedicated pager EC in core.

  Each pager EC consumes a thread context within core. Since core's
  thread-context area is limited, the maximum number of ECs within core is
  limited too. Because one core EC is needed as pager for each thread outside
  of core, the available thread contexts within core become a limited resource
  shared by all CPU-session clients. Because each Genode component is a client
  of core's CPU service, this bounded resource is effectively shared among all
  components. Consequently, the allocation of threads on NOVA's version of
  core represents a possible covert storage channel.

:Bounded number of object capabilities within core:

  For each capability created via core's CAP service, core allocates the
  corresponding NOVA portal and maintains the portal's capability selector
  during the lifetime of the associated object identity. Each allocation of
  a capability via core's CAP service consumes one entry in core's capability
  space. Because the space is bounded, clients of the CAP service could misuse
  core's capability space as covert storage channel.

:Core must retain mappings to all memory used throughout the system:

  As mentioned in Section [Page-fault handing on NOVA], core needs to own
  a mapping node before delegating the mapping to another PD as a response
  to a page fault. Otherwise, core could not revoke the mapping later on
  because the kernel expects core's mapping node as a proof for the
  authorization of the revocation of the mapping.

  Consequently, even though core never touches memory handed out to other
  components, it needs to have memory mappings with full access rights
  installed within its virtual address space.


; XXX further possible topics
; * Locking
; * Introduce the threads running within core
;   (why to use separate a separate thread for signals?)
; * Shared interrupts

